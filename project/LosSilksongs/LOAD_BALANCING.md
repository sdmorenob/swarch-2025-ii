# üéØ Implementaci√≥n de Balanceo de Carga - MusicShare

## üìã Resumen Ejecutivo

Se ha implementado con √©xito un sistema de **balanceo de carga autom√°tico** en MusicShare utilizando Traefik como API Gateway y Load Balancer. El sistema permite escalar horizontalmente los microservicios backend para mejorar el rendimiento, disponibilidad y resiliencia.

Se ha migrado la arquitectura a **Kubernetes**, reemplazando Docker Compose. El balanceo de carga se realiza autom√°ticamente mediante:
 - **Kubernetes Service Load Balancing**: Distribuci√≥n autom√°tica a trav√©s de Service Discovery
 - **HorizontalPodAutoscaler (HPA)**: Escalado autom√°tico basado en uso de CPU
 - **Traefik Gateway**: Enrutamiento inteligente de tr√°fico via IngressRoute CRDs
---

## üèóÔ∏è Arquitectura Kubernetes
### 1. üîß Configuraci√≥n de Docker Compose
```
Internet
  ‚Üì
Load Balancer P√∫blico (Service: frontend-loadbalancer)
  ‚Üì
Frontend React (Deployment 3 r√©plicas)
  ‚Üì
Traefik Gateway (Deployment 2 r√©plicas, ClusterIP)
  ‚Üì
Microservicios con Escalado Autom√°tico (HPA 2-6 r√©plicas seg√∫n CPU)
```

## üöÄ Componentes Escalables en Kubernetes
**Servicios Escalables:**
 | Servicio | R√©plicas Iniciales | M√°x (HPA) | Umbral CPU |
 |----------|-------------------|-----------|-----------|
 | UserService | 2 | 6 | 50% |
 | MusicService | 2 | 6 | 50% |
 | SocialService | 2 | 5 | 55% |
 | NotificationService | 2 | 6 | 50% |
 | Frontend | 3 | 3 | (sin HPA) |
 | Traefik Gateway | 2 | 2 | (sin HPA) |
- ‚úÖ **NotificationService**: 2 r√©plicas iniciales
**Cambios Realizados:**
- Pol√≠ticas de reinicio autom√°tico

**Ejemplo de Configuraci√≥n:**
```yaml
deploy:
  resources:
      memory: 512M
      cpus: '0.25'
      memory: 256M
  restart_policy:
    condition: on-failure
    delay: 5s
    max_attempts: 3
```

### 2. ‚öñÔ∏è Configuraci√≥n de Traefik

**Caracter√≠sticas Implementadas:**

#### Health Checks
```yaml
  - "traefik.http.services.userservice.loadbalancer.healthcheck.interval=10s"
```
### 1. üîß Configuraci√≥n de Kubernetes Deployments

**Servicios Escalables (HPA habilitado):**
 - ‚úÖ **UserService**: 2-6 r√©plicas
 - ‚úÖ **MusicService**: 2-6 r√©plicas  
 - ‚úÖ **SocialService**: 2-5 r√©plicas
 - ‚úÖ **NotificationService**: 2-6 r√©plicas

**Ejemplo de Deployment con recursos limitados:**
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: userservice
  namespace: musicshare
spec:
  replicas: 2  # R√©plicas iniciales
  selector:
    matchLabels:
      app: userservice
  template:
    metadata:
      labels:
        app: userservice
    spec:
      containers:
        - name: userservice
          image: musicshare/userservice:latest
          ports:
            - containerPort: 8002
          resources:
            requests:
              cpu: 250m
              memory: 256Mi
            limits:
              cpu: 500m
              memory: 512Mi
```

### 1.5. üîß Configuraci√≥n de HorizontalPodAutoscaler (HPA)

El escalado autom√°tico se configura mediante **HPA**, que monitorea m√©tricas de CPU y ajusta el n√∫mero de r√©plicas din√°micamente:

**Ejemplo de HPA para UserService:**
```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: userservice-hpa
  namespace: musicshare
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: userservice
  minReplicas: 2      # M√≠nimo 2 r√©plicas siempre
  maxReplicas: 6      # M√°ximo 6 r√©plicas
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 50  # Si CPU > 50%, escala hacia arriba

**C√≥mo funciona:**
 1. Metrics Server monitorea el uso de CPU en cada Pod
 2. Si uso promedio de CPU > 50%, el HPA crea nuevas r√©plicas
- Verificaci√≥n autom√°tica cada 10 segundos
```yaml
```
- Mantiene sesiones de usuario en la misma r√©plica
- **Round Robin** (por defecto)
  ### 2. ‚öñÔ∏è Configuraci√≥n de Traefik en Kubernetes

  **Caracter√≠sticas Implementadas:**

  #### IngressRoute CRD para Enrutamiento
  ```yaml
  apiVersion: traefik.io/v1alpha1
  kind: IngressRoute
  metadata:
    name: userservice-route
    namespace: musicshare
  spec:
    entryPoints:
      - web
      - websecure
    routes:
      - match: PathPrefix(`/api/users`)
        kind: Rule
        middlewares:
          - name: strip-users
        services:
          - name: userservice
            port: 8002
    tls:
      certResolver: letsencrypt-prod  # TLS autom√°tico
  ```

   - **Service Discovery**: Kubernetes API autom√°ticamente detecta cambios en Services
   - **Load Balancing**: Traefik distribuye tr√°fico a todos los Pods de un Deployment
   - **Health Checks**: Kubernetes liveness/readiness probes integrados

  #### Middleware para StripPrefix
  ```yaml
  apiVersion: traefik.io/v1alpha1
  kind: Middleware
  metadata:
    name: strip-users
    namespace: musicshare
  spec:
    stripPrefix:
      prefixes:
        - /api/users
  ```

   - Elimina el prefijo `/api/users` antes de pasar la solicitud al servicio
   - Permite que los servicios reciban rutas limpias (ej. `/me` en lugar de `/api/users/me`)

  #### Algoritmo de Balanceo
   - **Kubernetes Services**: Round-robin de Kubernetes a nivel DNS/iptables
   - **Traefik**: Distribuye equitativamente entre Pods saludables
   - **Session Affinity**: Opcional via `sessionAffinity: ClientIP` en Service

  #### Logs y M√©tricas
   - **Logs estructurados**: JSON enviados a `/var/log/traefik/`
   - **M√©tricas Prometheus**: Traefik expone m√©tricas en puerto 8080
   - **Dashboard**: Accesible en `http://localhost:8080/dashboard/`
   - **Integraci√≥n**: Prometheus scrape autom√°tico via ServiceMonitor (si usas Prometheus Operator)
- Distribuci√≥n equitativa entre r√©plicas saludables

#### Logs y M√©tricas
- Logs en formato JSON
- M√©tricas de Prometheus habilitadas
- Dashboard web en puerto 8080

### 3. üõ†Ô∏è Scripts de Automatizaci√≥n

#### scale-service.ps1
Script PowerShell para escalar servicios din√°micamente.


**Uso:**
```powershell
.\scripts\scale-service.ps1 -Service userservice -Replicas 5
.\scripts\scale-service.ps1 -Service all -Replicas 3
```

#### load-test.ps1
Script para probar el balanceo de carga mediante peticiones HTTP.


**Uso:**
```powershell
.\scripts\load-test.ps1 -Service userservice -Requests 20 -Delay 500
```

### 3. üõ†Ô∏è Comandos Kubernetes para Escalado Manual

**Escalar un servicio manualmente (sin HPA):**
```bash
# Escalar UserService a 5 r√©plicas
kubectl scale deployment userservice -n musicshare --replicas=5

# Escalar todos los servicios
kubectl scale deployment -n musicshare --all --replicas=3
```

**Monitorear escalado autom√°tico:**
```bash
# Ver estado del HPA
kubectl get hpa -n musicshare -w  # -w para watch (monitoreo en tiempo real)

# Detalles del HPA
kubectl describe hpa userservice-hpa -n musicshare

# Ver m√©tricas de CPU en tiempo real
kubectl top pods -n musicshare
kubectl top nodes
```

**Deshabilitar HPA (para pruebas):**
```bash
# Pausar el HPA
kubectl patch hpa userservice-hpa -n musicshare -p '{"spec":{"minReplicas":2,"maxReplicas":2}}'

# Eliminar HPA (vuelve al n√∫mero de r√©plicas del Deployment)
kubectl delete hpa userservice-hpa -n musicshare
```
### 4. üìö Documentaci√≥n

**Archivos Actualizados:**


## üöÄ C√≥mo Usar el Sistema

### Iniciar el Sistema con R√©plicas

```powershell
# Construir y levantar todos los servicios
docker compose build
docker compose up -d

# Verificar que las r√©plicas est√°n corriendo
### 4. üìö Documentaci√≥n Kubernetes

**Archivos Nuevos:**
 - ‚úÖ `k8s/TRAEFIK_SETUP.md` - Gu√≠a detallada de instalaci√≥n de Traefik
 - ‚úÖ `k8s/traefik-crd.yaml` - Custom Resource Definitions
 - ‚úÖ `k8s/traefik-config.yaml` - ConfigMap con configuraci√≥n
 - ‚úÖ `k8s/traefik-deployment-updated.yaml` - Deployment + RBAC
 - ‚úÖ `k8s/ingressroutes.yaml` - Rutas y middlewares
 - ‚úÖ `k8s/backend-deployments-services.yaml` - Microservicios
 - ‚úÖ `k8s/hpa.yaml` - Escalado autom√°tico
 - ‚úÖ `APIGateway.md` - Actualizado para Kubernetes
 - ‚úÖ `LOAD_BALANCING.md` - Actualizado con HPA

---

## üöÄ C√≥mo Desplegar en Kubernetes

### Pruebas de Carga y Escalado en Kubernetes

**Generar carga para activar escalado autom√°tico:**
```bash
# Port-forward al servicio
kubectl port-forward -n musicshare svc/userservice 8002:8002 &

# Usar herramienta como ab (Apache Bench) o wrk
# Instalar: brew install httpd (macOS) o apt-get install apache2-utils (Linux)
ab -n 10000 -c 100 http://localhost:8002/health

# Monitorear escalado en otra terminal
kubectl get hpa -n musicshare -w
```

**Ejemplo de salida esperada:**
```
NAME                REFERENCE                        TARGETS    MINPODS   MAXPODS   REPLICAS   AGE
userservice-hpa     Deployment/userservice           75%/50%    2         6         4          2m
# CPU sube a 75%, HPA escala de 2 a 4 r√©plicas
```

**Ver logs de escalado:**
```bash
kubectl get events -n musicshare --sort-by='.lastTimestamp' | tail -20
```

### Requisitos previos
 1. Cl√∫ster Kubernetes (minikube, kind, EKS, GKE, AKS, etc.)
 2. `kubectl` configurado
 3. `helm` (opcional, para cert-manager)
 4. Im√°genes Docker publicadas en un registry

### Despliegue paso a paso

```bash
# 1. Crear namespace y recursos de Traefik
kubectl apply -f k8s/namespace.yaml
kubectl apply -f k8s/traefik-crd.yaml
kubectl apply -f k8s/traefik-config.yaml
kubectl apply -f k8s/traefik-deployment-updated.yaml
kubectl apply -f k8s/ingressroutes.yaml

# 2. Desplegar servicios
kubectl apply -f k8s/frontend-deployment-service.yaml
kubectl apply -f k8s/backend-deployments-services.yaml
kubectl apply -f k8s/databases.yaml
kubectl apply -f k8s/hpa.yaml

# 3. Verificar despliegue
kubectl get all -n musicshare
kubectl get hpa -n musicshare
```

### Despliegue (m√©todo antiguo con Docker Compose)

Si todav√≠a usas Docker Compose (no recomendado, solo para desarrollo local):

```bash
# Construir y levantar todos los servicios
docker compose build
docker compose up -d
```
docker compose ps
```

### Escalar Servicios Manualmente

```powershell
# M√©todo 1: Docker Compose directo
docker compose up -d --scale userservice=5 --no-recreate

# M√©todo 2: Script (Recomendado)
.\scripts\scale-service.ps1 -Service userservice -Replicas 5

# Escalar todos los servicios
.\scripts\scale-service.ps1 -Service all -Replicas 3
```

### Probar el Balanceo de Carga

```powershell
# Ejecutar prueba de carga
.\scripts\load-test.ps1 -Service userservice -Requests 20 -Delay 500

## üì° Observabilidad Simplificada: Prometheus + Grafana

Se redujo la pila a un √∫nico sistema de m√©tricas (Prometheus) y visualizaci√≥n (Grafana):

- Prometheus scrapea `/metrics` de Traefik (expuesto en el puerto interno 8080 del contenedor).
- Grafana consume Prometheus como datasource √∫nico (UID sugerido para dashboard: `ms-trfk-lb-20251117`).

### Servicios involucrados

- `prometheus` (puerto 9090 ‚Üí http://localhost:9090)
- `grafana` (puerto 3010 ‚Üí http://localhost:3010)
- `traefik` (dashboard interno 8080, m√©tricas en `/metrics`)

### Levantar observabilidad

```powershell
docker compose up -d traefik prometheus grafana
docker compose ps traefik prometheus grafana
start http://localhost:3010
start http://localhost:9090
```

### Prometheus configuraci√≥n (archivo `prometheus/prometheus.yml`)
```yaml
global:
  scrape_interval: 10s
scrape_configs:
  - job_name: 'traefik'
    metrics_path: /metrics
    static_configs:
      - targets: ['traefik:8080']
```

### M√©tricas √∫tiles de Traefik
- `traefik_entrypoint_requests_total` por entrypoint
- `traefik_router_requests_total` por router (ruta l√≥gica)
- `traefik_service_requests_total` por servicio backend
- `traefik_service_request_duration_seconds_bucket` para latencias (usar p95/p99 v√≠a histogram quantiles)

### Panel b√°sico recomendado (Grafana)
1. Requests por router (panel tipo time series): `sum by(router) (rate(traefik_router_requests_total[1m]))`
2. Latencia p95 global: `histogram_quantile(0.95, sum(rate(traefik_service_request_duration_seconds_bucket[5m])) by (le))`
3. Errores 5xx por servicio: `sum by(service) (increase(traefik_service_requests_total{code=~"5.."}[5m]))`
4. Throughput total: `sum(rate(traefik_entrypoint_requests_total[1m]))`

---
## üß™ Prueba de Carga √önica (PowerShell)

Se dej√≥ solo el script `scripts/load-test.ps1` para pruebas manuales de distribuci√≥n y latencia b√°sico.

### Ejecutar prueba
```powershell
./scripts/load-test.ps1 -Service userservice -Requests 30 -Delay 300
```

Cambiar servicio:
```powershell
./scripts/load-test.ps1 -Service music-service -Requests 50 -Delay 200
```

### Qu√© validar
1. Respuestas HTTP 200 predominantes
2. Tiempos promedio estables < 500ms (desarrollo)
3. Distribuci√≥n entre r√©plicas (si sticky sessions no monopoliza la misma). Para mejor dispersi√≥n, repetir ejecuciones nuevas.

### Ejemplo interpretaci√≥n
```
‚úì 30/30 exitosas | p95 180ms | R√©plica A 16 / R√©plica B 14 ‚Üí balance ok
```

### Escalar y volver a probar
```powershell
docker compose up -d --scale userservice=4 --no-recreate
./scripts/load-test.ps1 -Service userservice -Requests 60 -Delay 200
```

---

# Resultado Esperado:
# R√©plica 1: ~33% (10 peticiones)
# R√©plica 2: ~33% (10 peticiones)
# R√©plica 3: ~33% (10 peticiones)
```

### Prueba 2: Failover Autom√°tico

```powershell
# Terminal 1: Ejecutar prueba continua
.\scripts\load-test.ps1 -Service userservice -Requests 100 -Delay 500

# Terminal 2: Durante la prueba, detener una r√©plica
docker compose ps userservice  # Identificar container ID
docker stop <container-id>

# Resultado Esperado:
# - Peticiones contin√∫an exitosas
# - Distribuci√≥n se ajusta autom√°ticamente
# - Dashboard muestra r√©plica como unhealthy
```

### Prueba 3: Escalado Bajo Carga

```powershell
# Terminal 1: Ejecutar prueba continua
.\scripts\load-test.ps1 -Service userservice -Requests 200 -Delay 500

# Terminal 2: Durante la prueba, escalar
.\scripts\scale-service.ps1 -Service userservice -Replicas 5

# Resultado Esperado:
# - Peticiones siguen funcionando
# - Nuevas r√©plicas se agregan autom√°ticamente
# - Traefik detecta y balancea a las nuevas r√©plicas
```

---

## üîê Consideraciones de Seguridad

### Implementadas ‚úÖ

1. **Sticky Sessions**: Mantiene sesi√≥n de usuario en misma r√©plica
2. **Health Checks**: Verifica estado antes de enviar tr√°fico
3. **Resource Limits**: Previene consumo excesivo de recursos
4. **TLS/HTTPS**: Todo el tr√°fico externo es cifrado
5. **Network Segmentation**: R√©plicas en redes aisladas

### Recomendadas para Producci√≥n ‚ö†Ô∏è

1. **Rate Limiting**: Limitar peticiones por IP
2. **Authentication**: Proteger dashboard de Traefik
3. **Certificados V√°lidos**: Usar Let's Encrypt o certificados corporativos
4. **Monitoring Avanzado**: Integrar Prometheus + Grafana
5. **Auto-scaling**: Migrar a Kubernetes para escalado autom√°tico

---

## üìà Pr√≥ximos Pasos

### Corto Plazo
- [ ] Implementar rate limiting por IP
- [ ] Agregar autenticaci√≥n al dashboard de Traefik
- [ ] Configurar alertas de Prometheus
- [ ] Crear scripts de backup autom√°tico

### Mediano Plazo
- [ ] Integrar Grafana para visualizaci√≥n
- [ ] Implementar circuit breakers
- [ ] Configurar auto-scaling basado en m√©tricas
- [ ] Agregar cach√© distribuido (Redis)

### Largo Plazo
- [ ] Migrar a Kubernetes para orquestaci√≥n avanzada
- [ ] Implementar service mesh (Istio/Linkerd)
- [ ] Multi-region deployment
- [ ] Disaster recovery automation

---

## üêõ Troubleshooting

### Las r√©plicas no se crean

**S√≠ntoma:** `docker compose up` no crea m√∫ltiples r√©plicas

**Causa:** Docker Compose requiere v2.x para soporte de `deploy.replicas`

**Soluci√≥n:**
```powershell
docker compose version  # Verificar versi√≥n
docker compose up -d --scale userservice=2  # Alternativa
```

### El balanceo no distribuye equitativamente

**S√≠ntoma:** Todas las peticiones van a la misma r√©plica

**Causa:** Sticky sessions habilitadas

**Soluci√≥n:** Esto es comportamiento esperado para mantener sesiones de usuario.
```powershell
# Probar sin cookies para ver distribuci√≥n real
.\scripts\load-test.ps1 -Service userservice -Requests 20
```

### Health checks fallan

**S√≠ntoma:** Dashboard muestra r√©plicas como "unhealthy"

**Causa:** Endpoint `/health` no disponible o servicio ca√≠do

**Soluci√≥n:**
```powershell
# Verificar logs
docker compose logs userservice

# Probar endpoint directamente
curl https://localhost/api/users/health -k

# Reiniciar servicio
docker compose restart userservice
```

### Consumo excesivo de recursos

**S√≠ntoma:** Sistema lento, alta utilizaci√≥n de CPU/RAM

**Causa:** Demasiadas r√©plicas o l√≠mites mal configurados

**Soluci√≥n:**
```powershell
# Ver consumo actual
docker stats

# Reducir r√©plicas
.\scripts\scale-service.ps1 -Service all -Replicas 2

# Ajustar l√≠mites en docker-compose.yml
```

---

## üìû Soporte

Para m√°s informaci√≥n:
- **Documentaci√≥n API Gateway**: [APIGateway.md](./APIGateway.md)
- **Documentaci√≥n Scripts**: [scripts/README.md](./scripts/README.md)
- **Dashboard Traefik**: http://localhost:8080/dashboard/
- **Repositorio**: https://github.com/JulianAVG64/MusicShare

---

## üìù Notas Finales

‚úÖ **Sistema completamente funcional** con balanceo de carga autom√°tico

‚úÖ **4 servicios escalables** con 2 r√©plicas iniciales cada uno

‚úÖ **Scripts automatizados** para operaciones comunes

‚úÖ **Documentaci√≥n completa** para uso y troubleshooting

‚úÖ **Health checks y failover** autom√°ticos implementados

‚úÖ **Sticky sessions** para mantener estado de usuario

---

*Implementaci√≥n completada el 17 de noviembre de 2025*  
*Versi√≥n: 1.0*  
*Equipo: Los SilkSongs*
