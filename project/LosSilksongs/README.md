# Project: Prototype 4
# MusicShare - Red Social Musical

- **[Aplicaci√≥n deplegada](https://musicshare.34.60.50.189.nip.io/login)**

## Tabla de Contenidos

* [**Team**](#team)
  * [Team name](#team)
  * [Full names and team members](#team)
* [**Software System**](#software-system)
  * [Name](#software-system)
  * [Logo](#software-system)
  * [Description](#software-system)
* [Functional Requirements](#functional-requirements)
* [Non-Functional Requirements](#non-functional-requirements)
* [**Architectural Structures**](#architectural-structures)
  * [Component-and Connector (C&C) Structure](#component-and-connector-cc-structure)
  * [Layered Structure](#layered-structure)
  * [Deployment Structure](#deployment-structure)
  * [Decomposition Structure](#decomposition-structure)
* [**Quality Attributes**](#quality-attributes)
  * [Security](#security)
  * [Performance and Scalability](#performance-and-scalability)
  * [Reliability](#reliability)
  * [Interoperability](#interoperability)
* [**Prototype**](#prototype)
  * [Implementing and deploying the software system prototype](#prototype)

---

## Team {#team}
- **Team name**: Los SilkSongs
- **Full name and team members**
  - Julian David Rodriguez Fernandez
  - Juli√°n Andr√©s Vargas Guti√©rrez
  - Gabriel Felipe Fonseca Guerrero
  - Gabriel Felipe Gonz√°lez Boh√≥rquez
  - Andr√©s Felipe Perdomo Uruburu
  - Andr√©s Felipe Poveda Bell√≥n

## Software System {#software-system}
 - **Name:** MusicShare
 - **Logo**

![Logo](Logo.jpg)

 
 - **Description**
**MusicShare** es una red social de m√∫sica desarrollada con una **arquitectura distribuida de microservicios**, que integra presentaci√≥n web en **React/TypeScript**, servicios de negocio independientes y bases de datos h√≠bridas (**PostgreSQL y MongoDB**). El sistema permite a los usuarios compartir y descubrir m√∫sica mientras garantiza **escalabilidad horizontal**, **baja latencia en streaming y alta disponibilidad**. La comunicaci√≥n entre componentes se gestiona mediante **REST, gRPC y WebSockets**, bajo un esquema seguro con **OAuth2/JWT y TLS 1.2+**. Todo el software se despliega en entornos contenedorizados con Docker/Kubernetes, con monitoreo centralizado, pruebas automatizadas y cumplimiento de est√°ndares de usabilidad, accesibilidad (WCAG 2.1 AA) y protecci√≥n de datos (GDPR/legislaci√≥n colombiana).

---

# Functional Requirements {#functional-requirements}
### RF01 - Gesti√≥n de Usuarios
### RF02 - Subida y Gesti√≥n de M√∫sica
- El sistema debe permitir subir archivos de audio (MP3, WAV) al cloud storage
- El sistema debe permitir agregar metadatos b√°sicos a las pistas (t√≠tulo, artista, g√©nero)
- El sistema debe permitir reproducir las pistas subidas
### RF03 - Feed Social Musical
- El sistema debe mostrar un timeline con las publicaciones musicales de usuarios seguidos
- El sistema debe permitir compartir pistas musicales como publicaciones
- El sistema debe mostrar informaci√≥n b√°sica de cada publicaci√≥n (usuario, fecha, t√≠tulo de la canci√≥n)
### RF04 - Sistema de Seguimiento
- El sistema debe permitir seguir y dejar de seguir otros usuarios
- El sistema debe mostrar la lista de seguidores y seguidos
- El sistema debe filtrar el feed basado en usuarios seguidos
### RF05 - Salas de M√∫sica Colaborativa - Para mirar para el MVP
- El sistema debe permitir crear salas de m√∫sica donde un usuario act√∫e como "DJ"
- El sistema debe permitir que otros usuarios se conecten a las salas creadas
- El sistema debe sincronizar la reproducci√≥n para todos los participantes de la sala
- El sistema debe mostrar qui√©n est√° conectado en cada sala
### RF06 - Interacciones B√°sicas
- El sistema debe permitir dar "like" a publicaciones musicales
- El sistema debe mostrar el contador de likes por publicaci√≥n
- El sistema debe permitir comentarios b√°sicos en las publicaciones
### RF07 - Descubrimiento Simple
- El sistema debe permitir explorar m√∫sica por g√©nero b√°sico
- El sistema debe mostrar publicaciones populares/trending
- El sistema debe permitir b√∫squeda simple por usuario o t√≠tulo de canci√≥n

## Non-Functional Requirements {#non-functional-requirements}

MusicShare es una aplicaci√≥n web que funciona como red social especializada donde los usuarios pueden compartir su m√∫sica favorita, crear playlists y descubrir nueva m√∫sica a trav√©s de una experiencia social interactiva.
### RNF-5.1: Dise√±o responsivo
Requisito: La interfaz de usuario web debe ser completamente responsiva y funcional en los principales tama√±os de pantalla: m√≥viles (320px-767px), tabletas (768px-1023px) y escritorio (1024px+).
M√©trica de aceptaci√≥n: Pruebas en emuladores de dispositivos y dispositivos f√≠sicos confirman que no hay elementos rotos o inutilizables en las resoluciones clave.
### RNF-5.2: Accesibilidad web
Requisito: La aplicaci√≥n debe cumplir con el nivel AA de las Pautas de Accesibilidad para el Contenido Web (WCAG 2.1).
M√©trica de Aceptaci√≥n: La aplicaci√≥n pasa las validaciones de herramientas automatizadas de accesibilidad (ej. Lighthouse, Axe) y supera una revisi√≥n manual de criterios clave (contraste, navegaci√≥n por teclado, texto alternativo para im√°genes).
Arquitectura y Distribuci√≥n
### RNF-1.1 Arquitectura Distribuida
El sistema debe seguir una arquitectura distribuida basada en microservicios, de modo que cada componente (frontend, servicios de negocio y bases de datos) pueda desplegarse y escalarse de manera independiente.
### RNF-1.2 Componentes de Presentaci√≥n
En el sistema la aplicaci√≥n web se desarrolla en React/TypeScript, de modo que interact√∫e con los servicios a trav√©s de conectores HTTP.
### RNF-1.3 Componentes de L√≥gica de Negocio
El sistema debe contar con un conjunto de componentes de l√≥gica, representados por microservicios independientes (UserService, MusicService, SocialService, SearchService, NotificationService, MetadataService) encargados de las distintas funcionalidades.
### RNF-1.4 Componentes de Datos
El sistema incluye componentes de datos de distinto tipo, espec√≠ficamente:
Base de datos relacional (PostgreSQL) para informaci√≥n estructurada de usuarios, relaciones sociales y metadatos clave.
Base de datos NoSQL (MongoDB) para almacenamiento de metadatos musicales, b√∫squeda y an√°lisis flexible.
Conectividad y Protocolos
### RNF-2.1 conectores basados en HTTP:
REST para operaciones CRUD y comunicaci√≥n est√°ndar entre frontend, gateway y microservicios.
WebSocket para notificaciones en tiempo real y actualizaciones del feed.
gRCP para soportar comunicaci√≥n entre microservicios internos para operaciones de alta frecuencia
### RNF-2.2 Conectividad con MongoDB
MongoDB Wire Protocol se encarga de la comunicaci√≥n entre la base de datos que guarda la m√∫sica y el componente que se encarga del servicio de m√∫sica
## Rendimiento y Escalabilidad
### RNF-3.1 Escalabilidad Horizontal:
La plataforma debe permitir el despliegue independiente de cada microservicio para escalar de manera horizontal seg√∫n la carga de usuarios, soportando picos de al menos 100 usuarios concurrentes. (toca discutir # de usuarios)
### RNF-3.2 Tiempo de Respuesta:
El tiempo promedio de respuesta de las API REST no debe superar 300 ms bajo una carga media, y 500 ms en picos de tr√°fico.
### RNF-3.3 Reproducci√≥n en Streaming:
La entrega de archivos de audio desde el Cloud Storage debe mantener una latencia inicial m√°xima de 2 s antes de iniciar la reproducci√≥n. (Desde el momento en que el usuario pone play hasta que empieza a sonar no deben pasar m√°s de 2 segundos)
Lenguajes y tecnolog√≠as
Se implementar√° el sistema de software en Python, Go y Java.
## Disponibilidad y Confiabilidad
### RNF-5.1: Tolerancia a Fallos
Requisito: La falla de un microservicio no cr√≠tico (ej. NotificationService) no debe afectar las funcionalidades principales del sistema, como la autenticaci√≥n, la subida y la reproducci√≥n de m√∫sica.
M√©trica de Aceptaci√≥n: Se realizan pruebas de caos (ej. deteniendo el contenedor de un servicio no cr√≠tico) y se verifica que las funciones principales siguen operativas.
### RNF-5.2 Modularidad e independencia:
La arquitectura de microservicios debe aislar fallos de un servicio sin afectar el funcionamiento global.
## Seguridad
### RNF-6.1 Autenticaci√≥n y Autorizaci√≥n:
Todos los endpoints deben requerir autenticaci√≥n mediante OAuth2, donde un servidor de autorizaci√≥n emite tokens de acceso en formato JWT. Dichos tokens deben incluir claims de roles y privilegios de usuario, que ser√°n validados en el gateway y en los microservicios para aplicar autorizaci√≥n basada en roles.
### RNF-6.2 Protecci√≥n de Datos:
Todo el tr√°fico entre cliente, gateway y microservicios debe viajar sobre HTTPS/TLS 1.2+.
### RNF-6.3 Almacenamiento Seguro:
Las contrase√±as en PostgreSQL deben almacenarse con bcrypt o algoritmo equivalente.
### RNF-6.4 Cumplimiento Legal:
El sistema debe cumplir con GDPR/LPD colombiana para la protecci√≥n de datos personales.
Mantenibilidad y Evoluci√≥n
### RNF-7.1 Despliegue Contenerizado:
Toda la infraestructura debe empaquetarse con Docker y ser orquestable mediante Docker Compose/Kubernetes, permitiendo CI/CD.
### RNF-7.2 Documentaci√≥n:
Cada servicio debe proveer documentaci√≥n de su API usando OpenAPI/Swagger actualizada.
### RNF-7.3 Pruebas Automatizadas:
Cobertura m√≠nima de 80 % en pruebas unitarias e integraci√≥n para cada microservicio.
## Compatibilidad e Interoperabilidad
### RNF-8.1 Navegadores Soportados:
El frontend debe funcionar en las √∫ltimas dos versiones estables de Chrome.
### RNF-8.1: Dise√±o Responsivo
Requisito: La interfaz de usuario web debe ser completamente responsiva y funcional en los principales tama√±os de pantalla: m√≥viles (320px-767px), tabletas (768px-1023px) y escritorio (1024px+).
M√©trica de Aceptaci√≥n: Pruebas en emuladores de dispositivos y dispositivos f√≠sicos confirman que no hay elementos rotos o inutilizables en las resoluciones clave.
### RNF-8.2: Accesibilidad Web
Requisito: La aplicaci√≥n debe cumplir con el nivel AA de las Pautas de Accesibilidad para el Contenido Web (WCAG 2.1).
M√©trica de Aceptaci√≥n: La aplicaci√≥n pasa las validaciones de herramientas automatizadas de accesibilidad (ej. Lighthouse, Axe) y supera una revisi√≥n manual de criterios clave (contraste, navegaci√≥n por teclado, texto alternativo para im√°genes).
## Usabilidad y Experiencia de Usuario
### RNF-9.1 Accesibilidad:
Cumplir con el nivel AA de WCAG 2.1, garantizando que personas con discapacidades visuales o motoras puedan usar el sistema.
### RNF-9.2 Responsividad:
La interfaz debe adaptarse a pantallas m√≥viles, tabletas y escritorios.
## Observabilidad y Monitoreo
### RNF-10.1 Logging Centralizado:
Todos los microservicios deben emitir logs en formato estructurado (JSON) y enviarlos a una plataforma central (ej. ELK/Prometheus + Grafana).
### RNF-10.2 M√©tricas de Salud:
Cada servicio expondr√° un endpoint /health para chequeos autom√°ticos por parte del orquestador y el API Gateway.

---

# Architectural Structures {#architectural-structures}
## Component-and Connector (C&C) Structure {#component-and-connector-cc-structure}
C&C View:
![C&C View](CyC_prototipo3.png)

## Description of architectural styles used.

- Microservicios: Servicios independientes con responsabilidades espec√≠ficas
- Microfrontends: Frontends independientes
- Layered Architecture: Separaci√≥n clara entre presentaci√≥n, l√≥gica y datos
- Event-Driven: Para notificaciones y actualizaciones en tiempo real
- API Gateway Pattern: Para enrutar requests y manejar autenticaci√≥n

## Description of architectural elements and relations 
## Componentes:
### Presentaci√≥n:
- Web Frontend (React/TypeScript): Interfaz de usuario principal
- Posts Frontend (JavaScript): Interfaz para la creaci√≥n de posts
### L√≥gica de Negocio:
- User Service (Python/FastAPI): Gesti√≥n de usuarios, autenticaci√≥n, perfiles
- Music Service (Go): Manejo de archivos musicales, metadatos, cloud storage
- Social Service (Java/Spring Boot): Feed, seguimientos, interacciones sociales
- Notification Service (Python): Sistema de notificaciones en tiempo real
- Search Service (Go): B√∫squedas y recomendaciones
- Metadata Service (Python/FastAPI): Obtenci√≥n de metadatos para las canciones subidas por medio de Music Service
### Datos:
- User Database (PostgreSQL): Datos de usuarios, perfiles, relaciones
- Music Metadata Database (MongoDB): Metadatos de canciones, playlists, tags
- Cloud Storage (AWS S3/Google Cloud): Archivos de audio
- Cache Layer (Redis): Cache para b√∫squedas y feed
## Conectores HTTP:
### REST API Connector:
  - Comunicaci√≥n entre Frontend y servicios
  - Operaciones CRUD est√°ndar
  - Autenticaci√≥n v√≠a JWT
### WebSocket Connector:
  - Notificaciones en tiempo real
  - Chat en vivo durante reproducciones
  - Updates del feed en tiempo real
### gRPC:
  - Conexi√≥n MusicService con MetadataService

## Layered Structure {#layered-structure}
### Layered View:
![Diagrama de capas](Diagrama_Capas_2.png)

### Vista de capas de la capa de negocios:

![Diagrama de capas de negocios](Capas_Business.png)

## Descripci√≥n de los Patrones Arquitect√≥nicos Utilizados

La arquitectura del sistema sigue el Patr√≥n Arquitect√≥nico en Capas (Layered Architectural Pattern), el cual organiza el software en niveles jer√°rquicos con responsabilidades bien definidas y relaciones unidireccionales tipo ‚Äúallowed-to-use‚Äù. Cada capa superior depende √∫nicamente de los servicios ofrecidos por la capa inmediatamente inferior, promoviendo as√≠ la modificabilidad, la escalabilidad y la separaci√≥n de responsabilidades.

Asimismo, se aplica el Patr√≥n de Microservicios dentro de la Capa de Negocio, donde cada servicio (User, Music, Social, Notification y Metadata) encapsula un dominio funcional espec√≠fico y se comunica mediante APIs REST o protocolos as√≠ncronos. Este enfoque permite el despliegue independiente, el aislamiento de fallos y una alta mantenibilidad.

Adem√°s, en la capa de presentaci√≥n se aplica el Patr√≥n de Micro Frontends, dividiendo la interfaz de usuario en dos aplicaciones independientes (Web Frontend y Posts Frontend). Cada una se despliega de manera aut√≥noma y consume los servicios del API Gateway. Este enfoque facilita la escalabilidad del frontend, el desarrollo paralelo por equipos distintos y la actualizaci√≥n independiente de m√≥dulos de interfaz sin afectar al resto del sistema.

Entre los patrones complementarios utilizados se encuentran:

Patr√≥n API Gateway: centraliza el acceso externo, el enrutamiento y la autenticaci√≥n hacia los servicios del backend.

Patr√≥n Base de Datos por Servicio (Database per Service): Cada microservicio gestiona su propia base de datos, garantizando independencia de datos.

## Descripci√≥n de los Elementos Arquitect√≥nicos y sus Relaciones

La arquitectura est√° compuesta por cinco capas l√≥gicas:

### Capa de Presentaci√≥n: 

Incluye los componentes orientados al usuario como Web Frontend y Posts Frontend. Estos m√≥dulos gestionan la interacci√≥n con el usuario, la visualizaci√≥n de datos y las peticiones al sistema. Se comunican exclusivamente con la Capa de Integraci√≥n mediante HTTP/REST.

### Capa de Integraci√≥n: 
Implementa el API Gateway, responsable del enrutamiento, balanceo de carga, autenticaci√≥n y control de tr√°fico. Act√∫a como una fachada que expone un punto de acceso unificado al frontend y delega las solicitudes hacia los microservicios correspondientes.

### Capa de Negocio (Business): 
Compuesta por microservicios independientes (User Service, Music Service, Social Service, Notification Service y Metadata Service). Cada uno encapsula reglas de negocio espec√≠ficas.

### Capa de Persistencia: 
Agrupa los componentes de almacenamiento de datos, como User Database (PostgreSQL), Music/Metadata Database (MongoDB), Social Database (PostgreSQL) y Cloud Storage para archivos multimedia. Cada microservicio accede exclusivamente a su propia fuente de datos.

### Capa de Infraestructura: 
Proporciona soporte de ejecuci√≥n y despliegue mediante Docker, Kubernetes, pipelines de CI/CD, monitoreo (Prometheus/Grafana) y gesti√≥n de logs (ELK). Esta capa sustenta a todas las dem√°s sin generar dependencias ascendentes.

Las relaciones entre capas son estrictamente descendentes (allowed-to-use), lo que asegura modularidad y evita dependencias circulares. Esta organizaci√≥n favorece el mantenimiento, permite reemplazar tecnolog√≠as en capas inferiores y facilita la escalabilidad independiente de los servicios.

## Deployment Structure {#deployment-structure}
Deployment View:
![Vista de despliegue](Despliegue_segmentado.png)

## Elementos Arquitect√≥nicos y Relaciones

### Visi√≥n General de la Arquitectura de Despliegue

El sistema est√° desplegado en **Google Cloud Platform (GCP)** utilizando una arquitectura de microservicios contenerizados orquestada por **Google Kubernetes Engine (GKE)**. El despliegue consiste en tres capas principales: capa de acceso externo, capa de orquestaci√≥n del cl√∫ster y capa de persistencia de datos.

### Capa 1: Acceso Externo y Gateway

**API Gateway (Externo)**
- **Componente:** NGINX Ingress Controller
- **Despliegue:** Servicio LoadBalancer con IP p√∫blica (34.60.50.189)
- **Responsabilidades:**
  - √önico punto de entrada para todo el tr√°fico externo
  - Terminaci√≥n TLS (HTTPS ‚Üí HTTP)
  - Enrutamiento HTTP de capa 7 basado en reglas de path y host
  - Balanceo de carga entre servicios internos
- **Relaciones:**
  - Expuesto a internet v√≠a GCP Network Load Balancer
  - Enruta tr√°fico a servicios ClusterIP internos dentro del namespace `musicshare`

**Gateway Container (Interno)**
- **Tecnolog√≠a:** Implementaci√≥n de gateway personalizada
- **Entorno de Ejecuci√≥n:** Contenedor NGINX
- **Responsabilidades:**
  - Enrutamiento y reenv√≠o de peticiones
  - Procesamiento de middleware
- **Relaciones:**
  - Comunica con frontend y microservicios backend v√≠a HTTP

### Capa 2: Cl√∫ster GKE - Servicios de Aplicaci√≥n

**Configuraci√≥n del Cl√∫ster:**
- **Tipo:** Cl√∫ster Kubernetes gestionado GKE
- **Zona:** us-central1-a
- **Node Pool:** 3 nodos worker (e2-medium: 2 vCPUs, 4GB RAM cada uno)
- **SO:** Container-Optimized OS (COS)
- **Recursos Totales:** 6 vCPUs, 12GB RAM

**Segmentaci√≥n por Namespace:**
- `musicshare`: Cargas de trabajo de aplicaci√≥n
- `ingress-nginx`: Controlador Ingress
- `cert-manager`: Gesti√≥n de certificados
- `kube-system`: Componentes del sistema Kubernetes

#### Servicios Frontend

**musicshare-frontend**
- **Tecnolog√≠a:** Servidor web NGINX
- **Contenedor:** Sirve aplicaci√≥n React est√°tica
- **Entorno de Ejecuci√≥n:** Runtime Node.js (build) ‚Üí NGINX (producci√≥n)
- **Escalado:** HPA habilitado (1-4 r√©plicas, CPU objetivo 50%)
- **Puerto:** 80
- **Relaciones:**
  - Accedido v√≠a Ingress Controller
  - Realiza llamadas HTTP REST a servicios backend
  - Comunica con user_service, social_service, music_service

**web_frontend (Next.js)**
- **Tecnolog√≠a:** Framework Next.js React
- **Entorno de Ejecuci√≥n:** Contenedor Node.js
- **Relaciones:**
  - Interfaz frontend alternativa
  - Mismo patr√≥n de comunicaci√≥n backend que frontend principal

#### Microservicios Backend

**musicshare-music-service**
- **Tecnolog√≠a:** Python 3.11 (Flask/FastAPI)
- **Contenedor:** musicshare-music-service
- **Entorno de Ejecuci√≥n:** Runtime Python 3.11, puerto 8081
- **R√©plicas:** 2 (redundancia activa-activa)
- **Responsabilidades:**
  - Gesti√≥n de cat√°logo musical
  - Manejo de metadatos de canciones
  - Integraci√≥n con metadata service v√≠a gRPC
- **Relaciones:**
  - Recibe peticiones HTTP REST desde frontend v√≠a Ingress
  - Realiza llamadas gRPC a metadata-service (puerto 50051)
  - Publica eventos a RabbitMQ
  - Consulta music_db (Cloud SQL)

**musicshare-social-service**
- **Tecnolog√≠a:** Java 21 (Spring Boot)
- **Contenedor:** musicshare-social-service
- **Entorno de Ejecuci√≥n:** JDK 21, puerto 8083
- **R√©plicas:** 2
- **Responsabilidades:**
  - Interacciones sociales (posts, comentarios, likes)
  - Feed de actividad de usuario
  - Gesti√≥n de grafo social
- **Relaciones:**
  - Recibe peticiones HTTP REST desde frontend
  - Publica eventos sociales a RabbitMQ (notificaciones)
  - Consulta social_db (Cloud SQL v√≠a sidecar proxy)
  - Descubre servicios v√≠a CoreDNS

**musicshare-metadata-service**
- **Tecnolog√≠a:** Python 3.11 (servidor gRPC)
- **Contenedor:** musicshare-metadata-service
- **Entorno de Ejecuci√≥n:** Runtime Python 3.11, puerto 50051
- **R√©plicas:** 2
- **Responsabilidades:**
  - Enriquecimiento de metadatos musicales (integraci√≥n API Spotify)
  - Artwork de √°lbumes e informaci√≥n de artistas
  - Proveedor de API gRPC
- **Relaciones:**
  - Recibe peticiones gRPC desde music-service
  - Comunica con API externa de Spotify
  - Consulta metadata_db (Cloud SQL)

**musicshare-mongodb**
- **Tecnolog√≠a:** MongoDB 7.0
- **Contenedor:** Imagen oficial MongoDB
- **Entorno de Ejecuci√≥n:** Servidor MongoDB
- **Puerto:** 27017
- **Responsabilidades:**
  - Almacenamiento basado en documentos para cat√°logo musical
  - Colecciones de artistas y √°lbumes
- **Relaciones:**
  - Accedido por music-service
  - Volumen persistente para durabilidad de datos

**musicshare-userservice**
- **Tecnolog√≠a:** Python 3.9 (Flask)
- **Contenedor:** musicshare-userservice
- **Entorno de Ejecuci√≥n:** Runtime Python 3.9, puerto 8082
- **R√©plicas:** 2
- **Responsabilidades:**
  - Autenticaci√≥n y autorizaci√≥n de usuarios
  - Generaci√≥n y validaci√≥n de tokens JWT
  - Gesti√≥n de perfiles de usuario
- **Relaciones:**
  - Recibe peticiones de autenticaci√≥n desde frontend
  - Consulta user_db (Cloud SQL)
  - Emite tokens JWT para Access Token Pattern

**notificationservice**
- **Tecnolog√≠a:** Python 3.9
- **Contenedor:** notificationservice
- **Entorno de Ejecuci√≥n:** Runtime Python 3.9, puerto 8082
- **Responsabilidades:**
  - Procesamiento as√≠ncrono de notificaciones
  - Consumidor de mensajes AMQP
  - Entrega de notificaciones push
- **Relaciones:**
  - Consume mensajes desde RabbitMQ
  - Consulta base de datos de notificaciones

#### Message Broker

**RabbitMQ**
- **Tecnolog√≠a:** Message broker RabbitMQ
- **Contenedor:** Imagen oficial RabbitMQ
- **Entorno de Ejecuci√≥n:** Runtime Erlang
- **Puerto:** 5672 (AMQP), 15672 (Management UI)
- **Responsabilidades:**
  - Enrutamiento as√≠ncrono de mensajes
  - Hub de comunicaci√≥n orientado a eventos
  - Implementaci√≥n de patr√≥n Pub/Sub
- **Relaciones:**
  - Publicadores: music-service, social-service
  - Consumidor: notificationservice
  - Descubrimiento de servicios v√≠a `AMQP_URL=amqp://rabbitmq:5672`

### Capa 3: Persistencia de Datos

**Instancia Cloud SQL (ms111rep)**
- **Tipo:** Google Cloud SQL para PostgreSQL
- **Versi√≥n:** PostgreSQL 15
- **M√©todo de Conexi√≥n:** Cloud SQL Proxy (Sidecar Pattern)
- **Red:** IP privada dentro de VPC de GCP, sin exposici√≥n p√∫blica
- **Bases de Datos:**
  - `user_db`: Cuentas de usuario y datos de autenticaci√≥n
  - `social_db`: Interacciones sociales, posts, comentarios
  - `metadata_db`: Cach√© de metadatos musicales
  - `music_db`: Cat√°logo musical (alternativa a MongoDB)
  - `restmark_db`: Datos de rese√±as/calificaciones

**Cloud SQL Proxy (Contenedores Sidecar)**
- **Tecnolog√≠a:** Google Cloud SQL Auth Proxy
- **Despliegue:** Contenedor sidecar en pods que requieren acceso a base de datos
- **Entorno de Ejecuci√≥n:** Namespace de red compartido del pod (localhost)
- **Autenticaci√≥n:** Workload Identity (basado en IAM, sin credenciales est√°ticas)
- **Puerto:** 5432 (protocolo PostgreSQL)
- **Relaciones:**
  - Co-ubicado con pods de userservice, social-service, metadata-service
  - Establece t√∫nel cifrado a instancia Cloud SQL
  - Aplicaci√≥n conecta a `localhost:5432`, sidecar hace proxy a Cloud SQL

### Aspectos Transversales

**Service Discovery (CoreDNS)**
- Registro autom√°tico de servicios basado en DNS
- Servicios comunican usando nombres DNS (ej: `http://metadata-service:50051`)
- Resoluci√≥n con √°mbito de namespace: `<service>.<namespace>.svc.cluster.local`

**Gesti√≥n de Certificados (cert-manager)**
- Ciclo de vida automatizado de certificados X.509
- Integraci√≥n con Let's Encrypt v√≠a protocolo ACME
- Solver de desaf√≠o HTTP-01
- Renovaci√≥n autom√°tica cada 90 d√≠as

**Balanceo de Carga (Multi-Nivel)**
- **L4 (GCP Network LB):** Distribuye tr√°fico TCP a nodos del cl√∫ster
- **L7 (NGINX Ingress):** Enrutamiento y balanceo basado en HTTP a servicios
- **Interno (kube-proxy):** Balanceo a nivel de pod v√≠a iptables/IPVS

**Controles de Seguridad**
- **Segmentaci√≥n de Red:** Servicios ClusterIP (sin IPs p√∫blicas) + LoadBalancer solo para Ingress
- **Cifrado:** TLS 1.2/1.3 para tr√°fico externo, opci√≥n mTLS para interno (no implementado)
- **Autenticaci√≥n:** Tokens JWT validados en cada microservicio
- **Gesti√≥n de Secretos:** Kubernetes Secrets para configuraci√≥n sensible

**Monitoreo y Observabilidad**
- **Metrics Server:** M√©tricas de utilizaci√≥n de recursos para HPA
- **Kubelet:** Monitoreo de salud de nodos y pods
- **Ingress Logs:** Logs de acceso para an√°lisis de tr√°fico

---



## Decomposition Structure
![Diagrama de descomposici√≥n de Dominio](general.png)

## Description {#decomposition-structure}
üéµ Estructura de Descomposici√≥n de Dominio ‚Äî MusicShare
Dominio Ra√≠z: MusicShare

Descripci√≥n general:
MusicShare es una plataforma colaborativa para compartir, reproducir y descubrir m√∫sica. El sistema est√° dise√±ado bajo una arquitectura basada en microservicios, donde cada dominio encapsula una funcionalidad espec√≠fica, comunic√°ndose entre s√≠ mediante un API Gateway.
Su estructura promueve la escalabilidad, la independencia de desarrollo y el despliegue modular de componentes.
Cliente para funcionalidades principales


### 1. frontend

![Frontend](frontendcorreccion.png)


- **Responsabilidad principal**:
  - Proporcionar la interfaz gr√°fica principal para los usuarios finales.
  - Es la capa de presentaci√≥n encargada de gestionar la interacci√≥n del usuario con las funcionalidades de la plataforma.

- **Funciones clave:**
  - Registro e inicio de sesi√≥n de usuarios.
  - Exploraci√≥n de canciones, playlists y perfiles.
  - Comunicaci√≥n directa con el API Gateway para consumir servicios REST.
  - Implementaci√≥n adaptable para navegadores web.

### 2. frontendSSR

![FrontendSSR](frontendSSRcorreccion.png)


- **Responsabilidad principal**:
  - Cliente con Server-Side Rendering que carga el formulario para enviar al cliente para crear los POST
- **Funciones clave:**
  - Permite arrastrar canciones
  - Inserci√≥n de Tags, 
  - Definir si es de tipo de p√∫blica, agrega descripci√≥n y hashtags

### 3. SocialService

![socialservice](socialservicecorreccion.png)

- **Responsabilidad principal:**
  - Encargado del componente social de la plataforma. Administra las interacciones, conexiones y actividades entre los usuarios.

- **Funciones clave:**
  - Manejo de publicaciones, comentarios y likes.
  - Seguimiento de usuarios (‚Äúfollowers/following‚Äù).
  - Integraci√≥n con el NotificationService para alertas sociales.
  - Conexi√≥n con UserService para obtener perfiles.

### 4. MusicService

![musicservice](musicservicecorreccion.png)

- **Responsabilidad principal:**
  - Administrar los recursos musicales y su ciclo de vida dentro del sistema.

- **Funciones clave:**
  - Almacenamiento y gesti√≥n de canciones y √°lbumes.
  - Control de derechos, autor√≠a y acceso.
  - Integraci√≥n con el MetadataService para obtener informaci√≥n descriptiva.
  - Exposici√≥n de endpoints para streaming o descarga.

### 5. Traekik

![traefik](traefikcorreccion.png)


## Apigateway
- **Responsabilidad principal:**
  - Centralizar y gestionar todas las solicitudes externas hacia los microservicios.
  - Act√∫a como punto √∫nico de entrada al ecosistema MusicShare.

-**Funciones clave**:
  - Seguridad, autenticaci√≥n y autorizaci√≥n.
  - Control de tr√°fico, logging y CORS.
  - Comunicaci√≥n entre frontends y los servicios internos.

## Load Balancer
- **Responsabilidad principal:**
  - Distribuir equitativamente las solicitudes entrantes entre m√∫ltiples instancias de un servicio.

-**Funciones clave**:
  - Garantizar alta disponibilidad del ecosistema MusicShare.
  - Garantizar escalabilidad del ecosistema MusicShare.

### 6. MetadataService

![metadataservice](metadataservicecorreccion.png)

- **Responsabilidad principal:**
  - Gestionar y proveer informaci√≥n descriptiva asociada al contenido musical.

- **Funciones clave:**
  - Procesamiento y almacenamiento de metadatos de audio (artista, √°lbum, duraci√≥n, g√©nero, etc.).
  - Indexaci√≥n de canciones para b√∫squeda y filtrado.
  - Soporte a MusicService y RecommendationService (si existiera).
  - Posible integraci√≥n con APIs externas para completar metadatos.

### 7. UserService

![userservice](userservicecorreccion.png)

- **Responsabilidad principal:**
  - Gestionar la informaci√≥n y autenticaci√≥n de los usuarios del sistema.

- **Funciones clave:**
  - Registro, login y recuperaci√≥n de contrase√±as.
  - Administraci√≥n de roles y permisos.
  - Exposici√≥n de informaci√≥n de perfil para otros servicios (SocialService, NotificationService).
  - Almacenamiento seguro de credenciales (posiblemente con JWT o OAuth2).

### 8. NotificationService

![notificationservice](notificationservicecorreccion.png)

- **Responsabilidad principal:**
  - Coordinar y enviar notificaciones a los usuarios seg√∫n eventos del sistema.

- **Funciones clave:**
  - Notificaciones por nuevas publicaciones, seguidores o reacciones.
  - Integraci√≥n con SocialService y UserService.
  - Env√≠o de notificaciones por correo, push o en la aplicaci√≥n.

Registro de eventos relevantes para los usuarios.

---

# QUALITY ATTRIBUTES {#quality-attributes}

## <u>Security</u> {#security}

### üîí Secure Channel Pattern

**Est√≠mulo:** Un usuario accede a la aplicaci√≥n MusicShare a trav√©s de internet desde su navegador.

**Respuesta:** El sistema establece una conexi√≥n HTTPS segura con certificado TLS v√°lido, cifrando toda la comunicaci√≥n entre cliente y servidor para proteger datos sensibles (credenciales, informaci√≥n de usuario) contra ataques de interceptaci√≥n (man-in-the-middle).

### Implementaci√≥n

**Patr√≥n Arquitect√≥nico:** Secure Channel Pattern

**T√°ctica Arquitect√≥nica:** Encrypt Data (cifrado de datos en tr√°nsito mediante TLS/SSL)

### Soluci√≥n T√©cnica

Se implement√≥ TLS Termination en el Ingress Controller de Kubernetes utilizando la siguiente arquitectura:

1. **Dominio con Magic DNS:** Uso de `nip.io` para resolver `musicshare.34.60.50.189.nip.io` a la IP p√∫blica del cl√∫ster, permitiendo la emisi√≥n de certificados v√°lidos sin necesidad de comprar un dominio.

2. **Automatizaci√≥n de Certificados:** Instalaci√≥n de `cert-manager` v1.13.3 en el cl√∫ster para gestionar autom√°ticamente el ciclo de vida de certificados X.509.

3. **Emisor Let's Encrypt:** Configuraci√≥n de un `ClusterIssuer` que utiliza el protocolo ACME de Let's Encrypt para obtener certificados gratuitos y renovarlos autom√°ticamente cada 90 d√≠as.

4. **Ingress con TLS:** Configuraci√≥n del recurso Ingress con:
   - Secci√≥n `tls` especificando el hostname y el secret donde se almacena el certificado
   - Anotaci√≥n `cert-manager.io/cluster-issuer` para activar la emisi√≥n autom√°tica
   - Solver HTTP-01 para validaci√≥n del dominio

**Resultado:** La aplicaci√≥n es accesible mediante HTTPS con certificado v√°lido, mostrando el candado de seguridad en navegadores sin advertencias. Todo el tr√°fico entre usuarios y la aplicaci√≥n viaja cifrado mediante TLS 1.2/1.3.

### Componentes de Seguridad
- **cert-manager:** Gestor de certificados autom√°tico
- **Let's Encrypt:** Autoridad Certificadora (CA) gratuita
- **NGINX Ingress Controller:** Punto de terminaci√≥n TLS
- **Secret Kubernetes:** Almacenamiento seguro del certificado y clave privada

### üõ°Ô∏è Reverse Proxy Pattern

### Escenario
**Est√≠mulo:** Un usuario externo env√≠a una petici√≥n HTTP/HTTPS hacia la aplicaci√≥n MusicShare desde internet.

**Respuesta:** El sistema intercepta la solicitud en un punto de entrada √∫nico, realiza terminaci√≥n TLS, oculta la topolog√≠a interna de microservicios y enruta la petici√≥n al servicio backend correspondiente bas√°ndose en reglas de capa 7 (HTTP).

### Implementaci√≥n

**Patr√≥n Arquitect√≥nico:** Reverse Proxy Pattern

**T√°ctica Arquitect√≥nica:** Limit Exposure (limitar exposici√≥n de servicios internos) y Limit Access (controlar acceso mediante punto de entrada √∫nico)

### Soluci√≥n T√©cnica

Se implement√≥ un proxy inverso utilizando NGINX Ingress Controller con las siguientes caracter√≠sticas:

1. **Punto de Entrada √önico:** NGINX Ingress Controller es el √∫nico componente con IP p√∫blica (LoadBalancer), actuando como gateway para todo el tr√°fico entrante.

2. **TLS Termination:** El proxy maneja el cifrado/descifrado HTTPS, liberando a los servicios backend de esta responsabilidad y centralizando la gesti√≥n de certificados.

3. **Enrutamiento Basado en Reglas:** Configuraci√≥n de recurso Ingress con reglas de enrutamiento por path y host, dirigiendo solicitudes a servicios internos seg√∫n URL (`/api/users` ‚Üí userservice, `/api/social` ‚Üí socialservice).

4. **Ocultamiento de Topolog√≠a:** Los clientes externos solo conocen el dominio p√∫blico; la estructura interna de microservicios, sus IPs y puertos permanecen invisibles.

**Resultado:** Aislamiento completo de servicios backend de acceso directo desde internet. Los logs del Ingress Controller muestran el flujo `cliente ‚Üí NGINX ‚Üí upstream (10.x.x.x)`, confirmando la mediaci√≥n del proxy en todas las comunicaciones.

### Componentes de Seguridad
- **NGINX Ingress Controller:** Proxy inverso y balanceador L7
- **Ingress Resource:** Definici√≥n de reglas de enrutamiento
- **LoadBalancer Service:** Exposici√≥n controlada del √∫nico punto de entrada
- **Upstream Routing:** Reenv√≠o interno a servicios ClusterIP

![Objeto Ingress](reverse_proxy_pattern.jpeg)
![Logs de acceso](reverse_proxy_pattern_2.jpeg)

### üõú Network Segmentation Pattern

### Escenario
**Est√≠mulo:** Un atacante intenta acceder directamente a microservicios internos o bases de datos desde internet, evitando el punto de entrada oficial.

**Respuesta:** El sistema deniega el acceso debido a la segmentaci√≥n de red implementada. Los servicios internos no tienen IPs p√∫blicas y residen en una red overlay privada, accesible √∫nicamente dentro del cl√∫ster y a trav√©s del Ingress Controller autorizado.

### Implementaci√≥n

**Patr√≥n Arquitect√≥nico:** Network Segmentation Pattern (DMZ + Internal Network)

**T√°ctica Arquitect√≥nica:** Segment Network (segmentar red en zonas de confianza) y Deploy in DMZ (desplegar componentes p√∫blicos en zona desmilitarizada)

### Soluci√≥n T√©cnica

Se implement√≥ segmentaci√≥n multinivel utilizando primitivas de red de Kubernetes:

1. **DMZ (Zona Desmilitarizada):** NGINX Ingress Controller desplegado con servicio tipo `LoadBalancer`, exponiendo √∫nicamente la IP p√∫blica necesaria para recibir tr√°fico HTTP/HTTPS.

2. **Red Interna Privada:** Todos los microservicios (userservice, socialservice, musicservice) y bases de datos (MongoDB, RabbitMQ) configurados con servicios tipo `ClusterIP`, sin IPs p√∫blicas asignadas (EXTERNAL-IP: `<none>`).

3. **Overlay Network:** Kubernetes proporciona una red virtual interna donde los servicios se comunican usando DNS interno y direcciones privadas (10.x.x.x), inaccesibles desde internet.

4. **Aislamiento por Namespace:** Uso del namespace `musicshare` para segregar l√≥gicamente los recursos de la aplicaci√≥n del resto del cl√∫ster (kube-system, ingress-nginx).

5. **Acceso Seguro a Cloud SQL:** Conexi√≥n a base de datos gestionada mediante Cloud SQL Proxy con t√∫nel cifrado, sin exponer la instancia SQL a la red p√∫blica.

**Resultado:** Superficie de ataque minimizada con un √∫nico punto de entrada. Es f√≠sicamente imposible acceder a microservicios o bases de datos directamente desde internet. La verificaci√≥n mediante `kubectl get svc` confirma que solo el Ingress Controller tiene EXTERNAL-IP asignada.

### Componentes de Seguridad
- **ClusterIP Services:** Servicios sin exposici√≥n p√∫blica
- **LoadBalancer Service:** √önico para Ingress Controller
- **Kubernetes Overlay Network:** Red virtual privada (CNI)
- **Namespace Isolation:** Segregaci√≥n l√≥gica `musicshare`
- **Cloud SQL Proxy:** T√∫nel cifrado para acceso a BD gestionada

![network_segmentation](network_segmentation_pattern.jpeg)
![network_segmentation_2](network_segmentation_pattern_2.jpeg)

### üîë Access Token Pattern (Escogido por el equipo)

**Est√≠mulo:** Un usuario autenticado realiza una acci√≥n sensible en la aplicaci√≥n (crear post, comentar, dar like) desde un microfrontend hacia diferentes microservicios.

**Respuesta:** El sistema valida la identidad del usuario mediante un token JWT firmado, extrae el `userId` de forma segura, y ejecuta la operaci√≥n sin requerir estado compartido entre servicios ni confiar en datos proporcionados por el cliente.

### Implementaci√≥n

**Patr√≥n Arquitect√≥nico:** Access Token Pattern

**T√°ctica Arquitect√≥nica:** Authenticate Users (autenticaci√≥n mediante tokens criptogr√°ficos) y Authorize Users (autorizaci√≥n basada en claims del token)

### Soluci√≥n T√©cnica

Se implement√≥ un esquema de autenticaci√≥n stateless basado en JWT con la siguiente arquitectura:

1. **Emisi√≥n de Tokens:** El microservicio `userservice` genera tokens JWT al validar credenciales en login, incluyendo claims esenciales (`userId`, `username`, `exp`) firmados criptogr√°ficamente.

2. **Propagaci√≥n del Token:** El cliente almacena el token y lo env√≠a en cada solicitud mediante el header HTTP `Authorization: Bearer <token>`.

3. **Validaci√≥n Descentralizada:** Cada microservicio implementa middleware de validaci√≥n que:
   - Verifica la firma criptogr√°fica usando clave secreta compartida
   - Comprueba expiraci√≥n del token
   - Extrae el `userId` para asociar acciones al usuario autenticado

4. **Autorizaci√≥n Impl√≠cita:** Las operaciones sensibles utilizan el `userId` extra√≠do del token validado, eliminando la necesidad de enviar identificadores desde el cliente y previniendo suplantaci√≥n de identidad.

**Resultado:** Autenticaci√≥n y autorizaci√≥n distribuida sin estado compartido, escalable para arquitecturas de microservicios. Los servicios validan independientemente cada solicitud (response time < 50ms por validaci√≥n), rechazando tokens inv√°lidos o expirados con c√≥digo HTTP 401.

### Componentes de Seguridad
- **JWT (JSON Web Tokens):** Tokens firmados con HS256 o RS256
- **Middleware de Validaci√≥n:** Interceptores en cada microservicio
- **Clave Secreta Compartida:** Almacenada en Secrets de Kubernetes
- **Token Expiration:** Configurado a 1 hora (renovable mediante refresh tokens)

---

## <u>Performance and Scalability</u> {#performance-and-scalability}

### ‚öñÔ∏è Load Balancer Pattern

**Est√≠mulo:** El sistema recibe un incremento significativo en el tr√°fico de usuarios concurrentes (de 10 a 500 solicitudes/segundo) debido a horarios pico o eventos especiales.

**Respuesta:** El sistema distribuye autom√°ticamente la carga entre m√∫ltiples instancias de servicios sin degradaci√≥n perceptible del rendimiento (response time < 500ms percentil 95), evitando sobrecarga de instancias individuales y maximizando la utilizaci√≥n de recursos disponibles.

### Implementaci√≥n

**Patr√≥n Arquitect√≥nico:** Load Balancer Pattern (Multi-Layer Load Balancing)

**T√°ctica Arquitect√≥nica:** Increase Available Resources (aumentar recursos mediante distribuci√≥n de carga)

### Soluci√≥n T√©cnica

Se implement√≥ una estrategia de balanceo de carga en tres niveles complementarios:

1. **Nivel 4 - Network Load Balancer (GCP):** El servicio `ingress-nginx` tipo `LoadBalancer` provisiona autom√°ticamente un balanceador de red TCP/UDP de Google Cloud Platform que distribuye tr√°fico entrante desde la IP p√∫blica (34.60.50.189) hacia los nodos worker del cl√∫ster GKE.

2. **Nivel 7 - Application Load Balancer (NGINX Ingress):** NGINX Ingress Controller act√∫a como balanceador de aplicaci√≥n HTTP/HTTPS, realizando:
   - TLS Termination centralizada
   - Enrutamiento basado en path y host
   - Distribuci√≥n de peticiones usando algoritmos Round Robin o Least Connections
   - Health checks a servicios backend

3. **Nivel Interno - Service Load Balancing (kube-proxy):** Los servicios tipo `ClusterIP` distribuyen tr√°fico entre m√∫ltiples r√©plicas de pods mediante iptables/IPVS:
   - Balanceo autom√°tico entre pods disponibles
   - Registro din√°mico de nuevas instancias al escalar
   - Exclusi√≥n autom√°tica de pods no saludables (failed readiness probes)

**Resultado:** Distribuci√≥n eficiente del tr√°fico en tres capas. El sistema escala horizontalmente mediante HPA (Horizontal Pod Autoscaler), creando nuevas r√©plicas que son autom√°ticamente integradas al pool de balanceo sin intervenci√≥n manual ni downtime.

### Componentes de Escalabilidad
- **GCP Network Load Balancer:** Balanceo L4 entre nodos del cl√∫ster
- **NGINX Ingress Controller:** Balanceo L7 con health checking
- **kube-proxy:** Balanceo interno entre r√©plicas de pods
- **Service Endpoints:** Registro din√°mico de pods disponibles

### ü™ú Auto Scaling Pattern (Escogido por el equipo)

**Est√≠mulo:** La carga del sistema aumenta progresivamente durante horas pico, incrementando el uso de CPU de los pods del frontend de 30% a 80% sostenido durante m√°s de 2 minutos.

**Respuesta:** El sistema detecta autom√°ticamente el incremento de carga mediante m√©tricas de utilizaci√≥n de recursos y escala horizontalmente el n√∫mero de r√©plicas del servicio frontend (de 1 a 4 pods), distribuyendo la carga y manteniendo el response time bajo (<200ms), sin intervenci√≥n manual ni downtime.

### Implementaci√≥n

**Patr√≥n Arquitect√≥nico:** Auto Scaling Pattern (Horizontal Pod Autoscaling)

**T√°ctica Arquitect√≥nica:** Introduce Concurrency (aumentar concurrencia mediante r√©plicas) y Resource Scheduling (planificaci√≥n din√°mica de recursos)

### Soluci√≥n T√©cnica

Se implement√≥ escalado horizontal autom√°tico utilizando Horizontal Pod Autoscaler (HPA) de Kubernetes:

1. **Metrics Server:** Recopila m√©tricas de uso de CPU y memoria de todos los pods en tiempo real, proporcionando datos al controlador de HPA.

2. **HPA Controller:** Configurado para el deployment `frontend` con los siguientes par√°metros:
   - **Target CPU:** 50% de utilizaci√≥n
   - **Min replicas:** 1 (estado en reposo)
   - **Max replicas:** 4 (l√≠mite para cl√∫ster e2-medium)
   - **Scale-up policy:** Crea nuevas r√©plicas cuando CPU > 50% durante 2+ minutos
   - **Scale-down policy:** Elimina r√©plicas cuando CPU < 50% durante 5+ minutos

3. **Integraci√≥n con Load Balancer:** Las nuevas r√©plicas creadas son autom√°ticamente registradas en el Service ClusterIP y comienzan a recibir tr√°fico balanceado inmediatamente tras pasar readiness probes.

4. **Resource Limits:** Cada pod tiene requests y limits de CPU/memoria definidos para garantizar c√°lculos precisos de utilizaci√≥n y evitar sobrecarga del nodo.

**Resultado:** Elasticidad autom√°tica basada en demanda real. Durante pruebas de carga, el HPA escal√≥ de 1 a 3 r√©plicas en ~90 segundos al detectar CPU > 50%, distribuyendo exitosamente la carga y previniendo degradaci√≥n del servicio. El sistema se auto-contrae en periodos de baja demanda, optimizando uso de recursos.

### Componentes de Escalabilidad
- **Horizontal Pod Autoscaler (HPA):** Controlador de escalado autom√°tico
- **Metrics Server:** Recolector de m√©tricas de recursos
- **Resource Requests/Limits:** Definici√≥n de recursos por pod
- **Readiness Probes:** Validaci√≥n de pods antes de recibir tr√°fico

![Muestra de AutoScaling 1](auto_scaling_pattern.jpeg)
![Muestra de AutoScaling 1](auto_scaling_pattern_2.jpeg)

---

## <u>Reliability</u> {#reliability}

### üóÑÔ∏è Replication Pattern

**Est√≠mulo:** Un nodo worker del cl√∫ster falla abruptamente debido a error de hardware o mantenimiento programado, afectando pods en ejecuci√≥n.

**Respuesta:** El sistema mantiene disponibilidad del servicio sin interrupci√≥n perceptible para los usuarios (downtime < 5 segundos). Kubernetes detecta la falla, evacua los pods del nodo problem√°tico y los recrea autom√°ticamente en nodos saludables, manteniendo el n√∫mero declarado de r√©plicas activas.

### Implementaci√≥n

**Patr√≥n Arquitect√≥nico:** Replication Pattern (Active-Active Redundancy)

**T√°ctica Arquitect√≥nica:** Active Redundancy (redundancia activa) y State Resynchronization (resincronizaci√≥n de estado)

### Soluci√≥n T√©cnica

Se implement√≥ replicaci√≥n horizontal a nivel de deployments de Kubernetes:

1. **Declaraci√≥n de R√©plicas:** Configuraci√≥n de `replicas: 2` (m√≠nimo) para microservicios cr√≠ticos (userservice, socialservice, musicservice) en manifiestos de deployment, garantizando m√∫ltiples instancias activas simult√°neas.

2. **Distribuci√≥n Multi-Nodo:** El scheduler de Kubernetes distribuye r√©plicas en diferentes nodos workers del cl√∫ster (3 nodos e2-medium), implementando anti-affinity impl√≠cita para maximizar tolerancia a fallos.

3. **Health Monitoring:** Configuraci√≥n de liveness y readiness probes que monitorizan continuamente el estado de cada r√©plica:
   - **Liveness probe:** Reinicia pods no responsivos
   - **Readiness probe:** Excluye r√©plicas no saludables del balanceo de carga

4. **Self-Healing:** El controlador ReplicaSet monitoriza constantemente el n√∫mero real vs deseado de r√©plicas. Si una r√©plica falla (pod crash, nodo down), autom√°ticamente programa una nueva instancia en un nodo disponible.

**Resultado:** Alta disponibilidad mediante redundancia activa. Durante pruebas de chaos engineering (simulaci√≥n de fallo de nodo), el servicio mantuvo disponibilidad con <5 segundos de impacto mientras Kubernetes reubicaba pods. Las r√©plicas restantes continuaron sirviendo tr√°fico sin degradaci√≥n gracias al load balancing.

### Componentes de Confiabilidad
- **ReplicaSet Controller:** Garantiza n√∫mero deseado de r√©plicas
- **Scheduler:** Distribuci√≥n inteligente de pods entre nodos
- **Health Probes:** Monitoreo continuo de estado de r√©plicas
- **Service Load Balancing:** Distribuci√≥n autom√°tica entre r√©plicas saludables

### üîç Service Discovery Pattern

**Est√≠mulo:** Un microservicio (userservice) necesita comunicarse con otro servicio interno (metadata-service) cuya direcci√≥n IP puede cambiar debido a reinicios, reescalados o migraciones entre nodos.

**Respuesta:** El sistema resuelve autom√°ticamente el endpoint actual del servicio destino mediante DNS interno, sin requerir configuraci√≥n manual de IPs ni reinicio de pods. La comunicaci√≥n se establece exitosamente usando nombres l√≥gicos estables independientemente de cambios en la topolog√≠a de red.

### Implementaci√≥n

**Patr√≥n Arquitect√≥nico:** Service Discovery Pattern (DNS-Based Discovery)

**T√°ctica Arquitect√≥nica:** Service Registry (registro centralizado de servicios) y Name Resolution (resoluci√≥n de nombres)

### Soluci√≥n T√©cnica

Se implement√≥ descubrimiento de servicios utilizando el sistema DNS nativo de Kubernetes:

1. **CoreDNS como Service Registry:** Kubernetes ejecuta CoreDNS como servidor DNS interno del cl√∫ster, manteniendo un registro actualizado autom√°ticamente de todos los servicios y sus endpoints.

2. **Nombres DNS Estables:** Cada Service ClusterIP obtiene un nombre DNS en formato `<service-name>.<namespace>.svc.cluster.local`, accesible mediante shortname dentro del mismo namespace (ej: `http://metadata-service:50051`).

3. **Resoluci√≥n Din√°mica:** Los pods consultan CoreDNS para resolver nombres de servicios. CoreDNS retorna la IP virtual (ClusterIP) del Service, que internamente balancea hacia pods backend disponibles mediante iptables/IPVS.

4. **Configuraci√≥n Basada en Variables:** Los microservicios usan variables de entorno con nombres l√≥gicos de servicios (ej: `AMQP_URL=amqp://rabbitmq:5672`), eliminando hardcoding de IPs y permitiendo portabilidad entre entornos.

**Resultado:** Desacoplamiento total entre consumidores y proveedores de servicios. Durante operaciones de scaling, updates o migraciones, los servicios contin√∫an comunic√°ndose sin modificar configuraciones. La resoluci√≥n DNS ocurre en <1ms, sin overhead perceptible en latencia.

### Componentes de Confiabilidad
- **CoreDNS:** Servidor DNS interno y service registry
- **Service ClusterIP:** Endpoints estables con IPs virtuales
- **DNS Resolution:** Traducci√≥n de nombres a IPs actuales
- **Environment Variables:** Configuraci√≥n portable de endpoints

### üñ•Ô∏è Cluster Pattern

**Est√≠mulo:** El sistema experimenta fallo total de un nodo worker, p√©rdida parcial de conectividad de red, o necesidad de mantenimiento sin ventana de downtime.

**Respuesta:** El cl√∫ster mantiene operatividad completa redistribuyendo carga entre nodos saludables. Los servicios permanecen disponibles gracias a la distribuci√≥n de r√©plicas en m√∫ltiples m√°quinas. El control plane orquesta recuperaci√≥n autom√°tica sin intervenci√≥n manual.

### Implementaci√≥n

**Patr√≥n Arquitect√≥nico:** Cluster Pattern (Distributed System Coordination)

**T√°ctica Arquitect√≥nica:** Voting (consenso distribuido v√≠a etcd) y Spare (recursos de respaldo distribuidos)

### Soluci√≥n T√©cnica

Se implement√≥ arquitectura de cl√∫ster completa utilizando Google Kubernetes Engine (GKE):

1. **Control Plane Gestionado:** GKE proporciona control plane de alta disponibilidad (etcd, API server, scheduler, controller manager) con multi-zona replication, garantizando continuidad de operaci√≥n orquestada.

2. **Worker Nodes Pool:** Cl√∫ster configurado con 3 nodos workers tipo e2-medium distribuidos en zona us-central1-a, proporcionando capacidad computacional agregada (6 vCPUs, 12GB RAM total).

3. **Workload Distribution:** El scheduler distribuye pods entre nodos usando algoritmos de resource balancing, evitando concentraci√≥n de carga cr√≠tica en un √∫nico nodo.

4. **Node Health Monitoring:** Kubelet en cada nodo reporta m√©tricas de salud al control plane. El node controller detecta nodos no responsivos (heartbeat timeout) y marca pods como no programables, iniciando reubicaci√≥n.

5. **Cluster-Level Networking:** CNI (Container Network Interface) implementa overlay network que permite comunicaci√≥n pod-to-pod transparente entre nodos, sobreviviendo a cambios de topolog√≠a.

**Resultado:** Tolerancia a fallos a nivel de infraestructura. El cl√∫ster opera como unidad l√≥gica √∫nica, ocultando complejidad de sistema distribuido a las aplicaciones. Durante fallo de nodo, los pods se reschedulean en ~30 segundos en nodos saludables, manteniendo disponibilidad general del sistema.

### Componentes de Confiabilidad
- **GKE Control Plane:** Orquestaci√≥n centralizada multi-zona
- **Multi-Node Pool:** Distribuci√≥n de carga entre 3 workers
- **etcd Cluster:** Base de datos distribuida con consenso Raft
- **Node Controller:** Monitoreo y recuperaci√≥n autom√°tica de nodos
- **CNI Overlay Network:** Conectividad resiliente entre nodos

### üèçÔ∏è Sidecar Pattern (Escogido por el equipo)

**Est√≠mulo:** Un microservicio necesita conectarse de forma segura a Cloud SQL (base de datos gestionada) que requiere autenticaci√≥n IAM, cifrado de conexi√≥n y no est√° expuesta p√∫blicamente.

**Respuesta:** El sistema establece conexi√≥n segura sin modificar el c√≥digo de la aplicaci√≥n principal. Un contenedor auxiliar maneja autom√°ticamente autenticaci√≥n, cifrado TLS y proxy de conexi√≥n, desacoplando l√≥gica de infraestructura de l√≥gica de negocio.

### Implementaci√≥n

**Patr√≥n Arquitect√≥nico:** Sidecar Pattern (Auxiliary Container)

**T√°ctica Arquitect√≥nica:** Increase Competence Set (aumentar capacidades sin modificar componente principal)

### Soluci√≥n T√©cnica

Se implement√≥ el patr√≥n Sidecar mediante contenedores auxiliares cloud-sql-proxy:

1. **Pod Multi-Container:** Configuraci√≥n de pods con dos contenedores que comparten ciclo de vida:
   - **Container principal:** Microservicio (userservice/socialservice) con l√≥gica de negocio
   - **Sidecar container:** cloud-sql-proxy que maneja conectividad a Cloud SQL

2. **Shared Network Namespace:** Ambos contenedores comparten stack de red (localhost), permitiendo que la aplicaci√≥n se conecte a `localhost:5432` mientras el sidecar gestiona el t√∫nel seguro hacia Cloud SQL.

3. **Responsabilidades del Sidecar:**
   - Autenticaci√≥n autom√°tica usando Workload Identity (IAM de GCP)
   - Establecimiento de t√∫nel TLS hacia instancia Cloud SQL privada
   - Renovaci√≥n autom√°tica de credenciales y reconexi√≥n ante fallos
   - Logging independiente de errores de conectividad

4. **Desacoplamiento de Infraestructura:** La aplicaci√≥n usa string de conexi√≥n est√°ndar PostgreSQL (`jdbc:postgresql://localhost:5432/db`), sin conocimiento de Cloud SQL, IAM o certificados. El sidecar abstrae completamente la complejidad de infraestructura.

**Resultado:** Conexi√≥n resiliente y segura a base de datos gestionada sin acoplamiento con c√≥digo de aplicaci√≥n. Si el sidecar falla, Kubernetes lo reinicia autom√°ticamente (shared pod lifecycle). La aplicaci√≥n obtiene conectividad cifrada, autenticada y con manejo autom√°tico de reconexi√≥n sin implementar esta l√≥gica internamente.

### Componentes de Confiabilidad
- **cloud-sql-proxy:** Contenedor sidecar especializado
- **Shared Network Namespace:** Comunicaci√≥n localhost entre contenedores
- **Workload Identity:** Autenticaci√≥n IAM sin credenciales est√°ticas
- **Automatic Reconnection:** Manejo de fallos de conectividad por sidecar

---

## <u>Interoperability</u> {#interoperability}

### üåâ Protocol Bridge Pattern

**Est√≠mulo:** M√∫ltiples microservicios implementados en diferentes tecnolog√≠as (.NET, Java Spring Boot, Python, Node.js) necesitan intercambiar datos y colaborar para completar operaciones de negocio end-to-end.

**Respuesta:** El sistema permite comunicaci√≥n transparente entre servicios heterog√©neos mediante protocolos est√°ndar de la industria (HTTP/REST, gRPC, AMQP). Los servicios intercambian datos sin conocimiento de las tecnolog√≠as de implementaci√≥n subyacentes, logrando interoperabilidad completa en arquitectura pol√≠glota.

### Implementaci√≥n

**Patr√≥n Arquitect√≥nico:** Protocol Bridge Pattern / API Gateway Pattern

**T√°ctica Arquitect√≥nica:** Orchestrate (orquestaci√≥n mediante protocolos est√°ndar) y Tailor Interface (adaptaci√≥n de interfaces seg√∫n requisitos de comunicaci√≥n)

### Soluci√≥n T√©cnica

Se implement√≥ interoperabilidad multi-protocolo utilizando est√°ndares abiertos:

1. **HTTP/REST para Comunicaci√≥n Cliente-Servidor:**
   - Frontend (React) ‚Üí Backend Services mediante API REST con JSON
   - Ingress Controller enruta requests HTTP bas√°ndose en path/host
   - Operaciones CRUD s√≠ncronas con verbos HTTP est√°ndar (GET, POST, PUT, DELETE)
   - Content negotiation mediante headers `Content-Type: application/json`

2. **gRPC para Comunicaci√≥n Inter-Servicio de Alto Rendimiento:**
   - musicservice ‚Üí metadata-service usando Protocol Buffers (protobuf)
   - Comunicaci√≥n binaria de baja latencia para operaciones s√≠ncronas cr√≠ticas
   - Configuraci√≥n mediante variable de entorno `METADATA_SERVICE_GRPC=metadata-service:50051`
   - Type-safe contracts definidos en archivos `.proto` compartidos

3. **AMQP para Mensajer√≠a As√≠ncrona:**
   - Servicios ‚Üí notificationservice mediante RabbitMQ como message broker
   - Configuraci√≥n mediante `AMQP_URL=amqp://rabbitmq:5672`
   - Patr√≥n Publish-Subscribe para eventos de dominio (nuevo post, nuevo comentario, like)
   - Desacoplamiento temporal: productores no esperan respuesta de consumidores

4. **Arquitectura Pol√≠glota:**
   - Servicios en diferentes stacks tecnol√≥gicos (.NET, Spring Boot, Python Flask, Node.js)
   - Comunicaci√≥n basada en contratos de protocolo, no en lenguaje de implementaci√≥n
   - Service mesh impl√≠cito mediante Kubernetes networking (CNI)

**Resultado:** Integraci√≥n seamless entre 8+ microservicios heterog√©neos sin dependencias de tecnolog√≠a. La selecci√≥n de protocolo (REST/gRPC/AMQP) se basa en requisitos de comunicaci√≥n: REST para APIs p√∫blicas, gRPC para llamadas s√≠ncronas de bajo overhead, AMQP para eventos as√≠ncronos. El sistema logra <100ms latencia promedio en llamadas inter-servicio gRPC y <50ms para REST.

### Componentes de Interoperabilidad
- **HTTP/REST:** API p√∫blica s√≠ncrona (frontend ‚Üî backend)
- **gRPC + Protobuf:** Comunicaci√≥n binaria de alto rendimiento (servicio-servicio)
- **AMQP (RabbitMQ):** Message broker para eventos as√≠ncronos
- **JSON/Protobuf:** Formatos de serializaci√≥n interoperables
- **Service Discovery:** Resoluci√≥n transparente de endpoints inter-servicio

---

# Gu√≠a de Despliegue MusicShare con NGINX Ingress Controller {#prototype}

### üìö Documentaci√≥n de Despliegue

- **[DEPLOYMENT_GUIDE.md](DEPLOYMENT_GUIDE.md)** - Gu√≠a paso a paso para desplegar
- **[LOAD_BALANCING.md](LOAD_BALANCING.md)** - Detalles de balanceo de carga
- **[APIGateway.md](APIGateway.md)** - Configuraci√≥n del API Gateway

## üìã Resumen

Esta gu√≠a describe c√≥mo desplegar MusicShare en Kubernetes usando **NGINX Ingress Controller** como API Gateway (reemplazando Traefik), proporcionando:

1. **LoadBalancer P√∫blico** ‚Üí Frontend React
2. **NGINX Ingress** ‚Üí API Gateway para microservicios
3. **Escalado Autom√°tico (HPA)** ‚Üí Servicios backend

## üîß Prerequisitos

- Kubernetes 1.24+ (minikube, kind, EKS, GKE, AKS, etc.)
- `kubectl` configurado
- Docker/Podman para construir im√°genes
- `helm` (opcional, para instalaciones avanzadas)
- `git`

## üì¶ Paso 1: Clonar Repositorio

```bash
git clone https://github.com/JulianAVG64/MusicShare.git
cd MusicShare
```

## üöÄ Paso 2: Preparar Im√°genes Docker

Aseg√∫rate de tener todas las im√°genes disponibles (en repositorio privado o local):

```bash
# Construir im√°genes localmente (si no est√°n en repositorio)
docker build -t musicshare/frontend:latest ./frontend/MusicShareFrontend/
docker build -t musicshare/userservice:latest ./userservice/
docker build -t musicshare/musicservice:latest ./musicservice/
docker build -t musicshare/social-service:latest ./socialservice/
docker build -t musicshare/notificationservice:latest ./notificationservice/
docker build -t musicshare/metadata-service:latest ./metadataservice/

# Si usas un registro privado:
docker tag musicshare/frontend:latest your-registry/musicshare/frontend:latest
docker push your-registry/musicshare/frontend:latest
# ... repetir para otros servicios
```

## üåç Paso 3: Crear Namespace

```bash
kubectl create namespace musicshare
kubectl label namespace musicshare name=musicshare
```

## üì• Paso 4: Instalar cert-manager (para HTTPS)

```bash
# Opci√≥n A: Con Helm
helm repo add jetstack https://charts.jetstack.io
helm repo update
helm install cert-manager jetstack/cert-manager \
  --namespace cert-manager \
  --create-namespace \
  --version v1.13.2

# Opci√≥n B: Con manifiestos directos
kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.13.2/cert-manager.yaml
```

## üîå Paso 5: Instalar NGINX Ingress Controller

### Opci√≥n A: Usando Kustomize (Recomendado)

```bash
# Solo NGINX Ingress
kubectl apply -f k8s/base/nginx-ingress-controller.yaml

# O con Kustomize (incluye cert-manager autom√°ticamente)
kubectl apply -k k8s/base/
```

### Opci√≥n B: Usando Helm

```bash
helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx
helm repo update

helm install nginx-ingress ingress-nginx/ingress-nginx \
  --namespace ingress-nginx \
  --create-namespace \
  --values - <<EOF
controller:
  replicaCount: 2
  resources:
    requests:
      cpu: 200m
      memory: 256Mi
    limits:
      cpu: 500m
      memory: 512Mi
  service:
    type: LoadBalancer
    annotations:
      service.beta.kubernetes.io/aws-load-balancer-type: "nlb"
EOF
```

## ‚úÖ Paso 6: Verificar NGINX Ingress

```bash
# Ver que el controller est√° running
kubectl get pods -n ingress-nginx
kubectl get svc -n ingress-nginx

# Obtener IP externa del LoadBalancer
kubectl get svc -n ingress-nginx nginx-ingress -w
# Espera a que aparezca la IP/hostname en EXTERNAL-IP
```

## üóÑÔ∏è Paso 7: Configurar Bases de Datos

Las bases de datos se crear√°n autom√°ticamente en el paso 8, pero puedes pre-crear vol√∫menes:

```bash
# Ver configuraci√≥n de bases de datos
kubectl apply -f k8s/app/databases.yaml

# Esperar a que est√©n ready
kubectl get pvc -n musicshare -w
```

## üéØ Paso 8: Desplegar MusicShare

### Opci√≥n A: Despliegue completo con Kustomize (Recomendado)

```bash
# Aplicar todo desde la carpeta k8s
kubectl apply -k k8s/

# Verificar que se est√°n creando recursos
kubectl get pods -n musicshare -w
kubectl get svc -n musicshare
kubectl get ingress -n musicshare
```

### Opci√≥n B: Despliegue paso a paso

```bash
# 1. Namespace y bases de datos
kubectl apply -f k8s/app/namespace.yaml
kubectl apply -f k8s/app/databases.yaml

# 2. Configuraci√≥n del frontend
kubectl apply -f k8s/app/frontend-config.yaml
kubectl apply -f k8s/app/frontend-deployment-service.yaml

# 3. Deployments y servicios backend
kubectl apply -f k8s/app/backend-deployments-services.yaml

# 4. API Gateway (NGINX Ingress)
kubectl apply -f k8s/app/ingress.yaml

# 5. Escalado autom√°tico
kubectl apply -f k8s/app/hpa.yaml

# 6. Cert-manager para HTTPS (si es necesario)
kubectl apply -f k8s/app/cert-manager-issuer.yaml
```

## üîç Paso 9: Verificar Despliegue

```bash
# Ver todos los pods
kubectl get pods -n musicshare -o wide

# Ver servicios
kubectl get svc -n musicshare

# Ver Ingress
kubectl get ingress -n musicshare -o wide

# Ver HPA (escalado autom√°tico)
kubectl get hpa -n musicshare

# Ver logs de un pod espec√≠fico
kubectl logs -n musicshare deployment/userservice --tail=100 -f

# Describir un pod (para ver errores)
kubectl describe pod -n musicshare <pod-name>
```

## üåê Paso 10: Obtener URLs de Acceso

```bash
# Frontend (LoadBalancer p√∫blico)
FRONTEND_IP=$(kubectl get svc -n musicshare frontend-loadbalancer -o jsonpath='{.status.loadBalancer.ingress[0].ip}')
echo "Frontend: http://$FRONTEND_IP"

# API Gateway (NGINX Ingress)
NGINX_IP=$(kubectl get svc -n ingress-nginx nginx-ingress -o jsonpath='{.status.loadBalancer.ingress[0].ip}')
echo "API Gateway: http://$NGINX_IP"
echo "  - User API: http://$NGINX_IP/api/users"
echo "  - Music API: http://$NGINX_IP/api/music"
echo "  - Social API: http://$NGINX_IP/api/social"
echo "  - Notifications API: http://$NGINX_IP/api/notifications"
echo "  - WebSocket: ws://$NGINX_IP/ws"

# NGINX Metrics (para Prometheus)
echo "NGINX Metrics: http://$NGINX_IP:10254/metrics"
```

## üß™ Paso 11: Pruebas B√°sicas

```bash
# Probar acceso al Frontend
curl -v http://$FRONTEND_IP/

# Probar API Gateway
curl -v http://$NGINX_IP/api/users/health

# Ver m√©tricas de NGINX
curl http://$NGINX_IP:10254/metrics

# Probar WebSocket
wscat -c ws://$NGINX_IP/ws
```

## üìä Paso 12: Configurar Monitoreo

### Prometheus (Recomendado)

```bash
# Verificar que prometheus.yml apunta a NGINX metrics
kubectl apply -f prometheus/prometheus.yml

# Agregar ServiceMonitor para NGINX (opcional)
kubectl apply -f - <<EOF
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: nginx-ingress
  namespace: ingress-nginx
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: ingress-nginx
  endpoints:
  - port: metrics
EOF
```

### Grafana

```bash
# Dashboard recomendado: ID 14314 (NGINX Ingress)
# https://grafana.com/grafana/dashboards/14314
```

## üîê Paso 13: Configurar HTTPS (Opcional)

```bash
# 1. Editar k8s/app/ingress.yaml y agregar secci√≥n `tls`
# 2. Usar cert-manager para provisionar certificados autom√°ticamente

kubectl apply -f - <<EOF
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: musicshare-tls
  namespace: musicshare
spec:
  secretName: musicshare-tls
  issuerRef:
    name: letsencrypt-prod
    kind: ClusterIssuer
  dnsNames:
  - musicshare.example.com
EOF
```

## üîÑ Paso 14: Pruebas de Carga y Escalado

```bash
# Instalar k6
curl https://github.com/grafana/k6/releases/download/v0.47.0/k6-v0.47.0-linux-amd64.tar.gz | tar xz

# Ejecutar pruebas
./k6 run k6/baseline.js

# Observar escalado autom√°tico
kubectl get hpa -n musicshare -w
kubectl get pods -n musicshare -w
```

## üìù Paso 15: Configurar Variables de Entorno

Los servicios usan variables de entorno. Verificar `k8s/app/backend-deployments-services.yaml`:

```yaml
env:
  - name: POSTGRES_HOST
    value: postgres
  - name: MONGODB_URI
    value: "mongodb://admin:password123@mongodb:27017/musicshare?authSource=admin"
  - name: NOTIFICATION_SERVICE_URL
    value: "http://notificationservice:8082"
  - name: USER_SERVICE_URL
    value: "http://userservice:8002"
```

**Cambiar contrase√±as en producci√≥n:**

```bash
# Crear Secret de Kubernetes
kubectl create secret generic db-credentials \
  -n musicshare \
  --from-literal=postgres-password=tu-password-seguro \
  --from-literal=mongodb-password=tu-password-seguro
```

## üõ†Ô∏è Troubleshooting

### Los pods no est√°n starting

```bash
# Ver eventos del cluster
kubectl describe nodes

# Ver logs del pod
kubectl logs -n musicshare <pod-name> --previous

# Ver descripci√≥n detallada
kubectl describe pod -n musicshare <pod-name>
```

### NGINX no redirige correctamente

```bash
# Ver configuraci√≥n generada de NGINX
kubectl exec -n ingress-nginx deployment/nginx-ingress-controller -- cat /etc/nginx/nginx.conf

# Verificar que el Ingress tiene rutas correctas
kubectl get ingress -n musicshare api-gateway -o yaml

# Logs de NGINX
kubectl logs -n ingress-nginx deployment/nginx-ingress-controller -f
```

### LoadBalancer sin IP externa

```bash
# En minikube/kind, usar port-forward
kubectl port-forward -n musicshare svc/frontend-loadbalancer 80:80 &
kubectl port-forward -n ingress-nginx svc/nginx-ingress 80:80 &

# En cloud providers, esperar a que se provisione
kubectl get svc -n musicshare frontend-loadbalancer -w
```

### WebSocket no funciona

```bash
# Verificar que NGINX tiene la anotaci√≥n correcta
kubectl get ingress -n musicshare api-gateway -o yaml | grep websocket

# Ver si el servicio est√° escuchando en puerto 8082
kubectl get svc -n musicshare notificationservice
```

## üóëÔ∏è Limpiar Recursos

```bash
# Eliminar MusicShare
kubectl delete -k k8s/

# Eliminar NGINX Ingress
kubectl delete -k k8s/base/

# Eliminar namespace
kubectl delete namespace musicshare

# Eliminar NGINX Ingress namespace
kubectl delete namespace ingress-nginx
```

## üìö Referencias √ötiles

- [NGINX Ingress Controller Docs](https://kubernetes.github.io/ingress-nginx/)
- [Kubernetes Ingress API](https://kubernetes.io/docs/concepts/services-networking/ingress/)
- [cert-manager Docs](https://cert-manager.io/)
- [Kubernetes Service Types](https://kubernetes.io/docs/concepts/services-networking/service/)
- [HorizontalPodAutoscaler](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/)

## ‚ú® Configuraci√≥n Recomendada para Producci√≥n

```bash
# 1. Usar certificados SSL/TLS reales
# 2. Habilitar autoscaling basado en m√©tricas reales
# 3. Configurar l√≠mites de recursos apropiados
# 4. Implementar network policies
# 5. Usar private container registry
# 6. Configurar backups autom√°ticos de bases de datos
# 7. Implementar monitoring y alerting
# 8. Usar pod security policies
# 9. Configurar RBAC adecuadamente
# 10. Implementar secrets management (Vault, AWS Secrets Manager, etc.)
```

## ‚ùì Soporte

Para problemas, consultar:
- Logs: `kubectl logs -n musicshare <pod-name>`
- Eventos: `kubectl get events -n musicshare`
- Descripci√≥n: `kubectl describe pod -n musicshare <pod-name>`
- Debugging: `kubectl debug -n musicshare <pod-name>`