# Laboratory 4 - Architectural Patterns

**Equipo:** Julian David Rodriguez fernandez, Andr√©s Felipe Perdomo Uruburu

---

**An√°lisis del Problema:**

El sistema de PagoGlobal sufre un problema cr√≠tico de agotamiento de recursos (Thread Pool Exhaustion). Durante el flash sale, las transacciones grandes que requieren validaci√≥n de fraude con FraudBlocker bloquearon todos los threads disponibles esperando respuestas lentas (15-20 segundos). Como no existe separaci√≥n de recursos, las transacciones peque√±as que no requieren validaci√≥n tambi√©n fueron rechazadas, causando una falla en cascada que tumb√≥ todo el sistema de pagos por 45 minutos.

Los problemas espec√≠ficos son:

- Blocking I/O sin aislamiento de recursos entre flujos cr√≠ticos y no cr√≠ticos
- Timeout muy alto (20 segundos) que mantiene threads bloqueados demasiado tiempo
- Ausencia de Circuit Breaker para detectar degradaci√≥n del servicio externo
- Sin estrategia de fallback cuando FraudBlocker falla

**Soluci√≥n Propuesta: Circuit Breaker + Bulkhead Pattern**

La soluci√≥n combina dos patrones arquitect√≥nicos:

1. **Bulkhead Pattern:** Separar recursos en pools aislados

   - Thread Pool 1: Transacciones < $100k (sin fraud check) - 50 threads
   - Thread Pool 2: Transacciones >= $100k (con fraud check) - 20 threads
   - Esto garantiza que transacciones peque√±as nunca se vean afectadas por problemas con FraudBlocker

2. **Circuit Breaker Pattern:** Detectar y manejar fallas del servicio externo

   - Configuraci√≥n: Abre el circuito si 50% de llamadas fallan o tardan m√°s de 2 segundos
   - Timeout agresivo de 2 segundos (vs 20 segundos original)
   - Estados: CLOSED (normal), OPEN (fallando), HALF_OPEN (probando recuperaci√≥n)
   - Cuando el circuito est√° abierto, usa fallback inmediatamente

3. **Fallback Strategy:** Plan B cuando FraudBlocker no est√° disponible
   - Opci√≥n A: Usar modelo de Machine Learning local (menos preciso pero funcional)
   - Opci√≥n B: Aprobar transacci√≥n con flag de "revisar manualmente despu√©s"
   - La transacci√≥n se encola para revisi√≥n posterior cuando FraudBlocker se recupere

**Arquitectura de la Soluci√≥n:**

```mermaid
sequenceDiagram
    participant Client
    participant SmallPool as Small TX Pool
    participant LargePool as Large TX Pool
    participant CB as Circuit Breaker
    participant FB as FraudBlocker
    participant Fallback as ML Model

    Client->>SmallPool: TX < $100k
    SmallPool->>Client: Process OK

    Client->>LargePool: TX >= $100k
    LargePool->>CB: Check State

    alt Circuit CLOSED
        CB->>FB: Call (2s timeout)
        FB->>CB: Response
        CB->>LargePool: Result
    else Circuit OPEN
        CB->>Fallback: Use Local ML
        Fallback->>CB: Prediction
        CB->>LargePool: Fallback Result
    end

    LargePool->>Client: Process Complete
```

**Resultados Esperados:**

- Transacciones peque√±as: 100% disponibilidad (no afectadas por FraudBlocker)
- Transacciones grandes: Degradaci√≥n elegante con fallback
- Tiempo de falla reducido de 20s a 2s
- Auto-recuperaci√≥n cuando FraudBlocker vuelve a funcionar
- P√©rdidas reducidas de miles de millones a pr√°cticamente cero

---

### Scenario 2: MiSalud Digital - Cross-Cutting Concerns Hell

**An√°lisis del Problema:**

MiSalud Digital tiene 70+ microservicios en m√∫ltiples lenguajes (Java, Python, Node.js) donde cada equipo implement√≥ funcionalidades transversales de forma inconsistente. Problemas cr√≠ticos que bloquean el lanzamiento:

1. **mTLS inconsistente:** Cada servicio maneja certificados y encriptaci√≥n de forma diferente
2. **Auditor√≠a fragmentada:** No hay forma unificada de loggear todas las llamadas entre servicios
3. **Canary deployments imposibles:** No existe infraestructura para rutear 1% del tr√°fico a versiones nuevas
4. **Resiliencia inconsistente:** Java usa Hystrix, Python tiene l√≥gica custom, Node.js otra diferente

El problema ra√≠z es que **cross-cutting concerns** (seguridad, observabilidad, resiliencia, traffic management) est√°n mezclados con la l√≥gica de negocio en cada servicio. Cualquier cambio requiere actualizar 70 servicios en 3+ lenguajes diferentes, coordinando deploys que toman semanas.

**Soluci√≥n Propuesta: Service Mesh (Istio)**

Un Service Mesh separa completamente los cross-cutting concerns de la l√≥gica de negocio usando el patr√≥n Sidecar Proxy:

1. **Sidecar Injection:** Istio inyecta autom√°ticamente un proxy (Envoy) en cada pod
2. **Traffic Interception:** El proxy intercepta TODO el tr√°fico entrante y saliente del servicio
3. **Services to Localhost:** Los servicios solo hablan a localhost sin saber de mTLS, retries, etc.
4. **Control Plane:** Istio configura todos los proxies centralizadamente con pol√≠ticas consistentes

**Soluciones Espec√≠ficas:**

**1. mTLS Autom√°tico:**

- Citadel genera y rota certificados autom√°ticamente
- Envoy maneja todo el handshake mTLS
- Los servicios hablan a localhost sin encriptaci√≥n
- 100% del tr√°fico encriptado sin cambiar una l√≠nea de c√≥digo

**2. Auditor√≠a Centralizada:**

- Envoy loggea autom√°ticamente source, destination, path, latency de TODAS las llamadas
- Logs inmutables enviados a sistema centralizado
- Cumple 100% con requisitos de auditor√≠a

**3. Canary Deployments (1% tr√°fico):**

- Traffic splitting configurado con YAML
- Cambiar porcentajes sin redesplegar servicios
- Rollback instant√°neo cambiando configuraci√≥n

**4. Resiliencia Consistente:**

- Misma pol√≠tica para TODOS los servicios que llaman a patient-records
- Independiente del lenguaje (Java, Python, Node.js)
- Configuraci√≥n centralizada y consistente

**Arquitectura de la Soluci√≥n:**

```mermaid
graph TB
    subgraph Control Plane
        CP[Istio Control Plane]
    end

    subgraph Pod A
        SA[Service A<br/>Java]
        PA[Envoy Proxy]
        SA -.localhost.-> PA
    end

    subgraph Pod B
        SB[Service B<br/>Python]
        PB[Envoy Proxy]
        SB -.localhost.-> PB
    end

    subgraph Pod C
        SC[Service C<br/>Node.js]
        PC[Envoy Proxy]
        SC -.localhost.-> PC
    end

    CP -.Config Push.-> PA
    CP -.Config Push.-> PB
    CP -.Config Push.-> PC

    PA == "mTLS Encrypted" ==> PB
    PB == "mTLS Encrypted" ==> PC
    PC == "mTLS Encrypted" ==> PA

```

**Resultados Esperados:**

- mTLS 100% autom√°tico en todos los servicios
- Auditor√≠a completa sin modificar c√≥digo
- Canary deployments con cambios de configuraci√≥n
- Pol√≠ticas de resiliencia consistentes
- Tiempo de actualizaci√≥n reducido de semanas a minutos
- Desbloqueo del lanzamiento de la plataforma

---

# üöÄ Scenario 3: EntregaRapida

## üìã Resumen Ejecutivo

**Problema**: Colapso del sistema por uso de descubrimiento est√°tico de IPs en un entorno Kubernetes din√°mico durante pico de carga.

**Soluci√≥n Inmediata**: Implementar Kubernetes Service + Migraci√≥n progresiva con Strangler Pattern.

---

## üéØ Metodo a implementar: metodo Strangler

## üéØ ¬øPOR QU√â EL M√âTODO STRANGLER?

### üîç El Problema Fundamental

```bash
# SISTEMA ACTUAL (ROTO)
Dispatch-Service (VM) ‚Üí Lista IPs est√°tica ‚Üí Routing-Service Pods (K8s)

# PROBLEMAS:
# ‚úÖ Escalado din√°mico de Routing-Service
# ‚ùå Descubrimiento est√°tico de IPs
# ‚ùå Acoplamiento fuerte legacy-cloud
# ‚ùå Punto √∫nico de falla

üåü ¬øPor Qu√© Strangler Pattern?

Ventajas para EntregaR√°pida:

‚úÖ Cero downtime durante la migraci√≥n

‚úÖ Reducci√≥n de riesgo - migraci√≥n por componentes

‚úÖ Aprendizaje progresivo del nuevo stack

‚úÖ Rollback instant√°neo en cualquier fase

‚úÖ Business continuity garantizada

FASE 1: ESTABILIZACI√ìN INMEDIATA (48 horas)


Beneficios inmediatos:

üöÄ Descubrimiento autom√°tico de pods

‚ö° Cero latencia en actualizaciones

üîÑ Load balancing nativo

üõ°Ô∏è Health checking autom√°tico


EVOLUCI√ìN DEL SISTEMA:
[FASE 0] Legacy Complete
    ‚Üì
[FASE 1] Strangler Fig Appears
    ‚Üì
[FASE 2] Coexistence
    ‚Üì
[FASE 3] New System Dominates
    ‚Üì
[FASE 4] Legacy Retired



MIGRACI√ìN POR CAPAS:
1. ‚úÖ Service Discovery (COMPLETADO - Fase 1)
2. üîÑ Basic Dispatch Logic
3. ‚è≥ Advanced Routing Features
4. ‚è≥ Driver Management
5. ‚è≥ Package Tracking



```
